---
layout: post
author: Colin Swaney
title: Gaussian Processes
date: 2022-01-18
categories: [research]
category: research
tags: [research]
excerpt: "<p></p>"
---

Given a sequence of points $$\mathbf{x}_1, \dots, \mathbf{x}_N$$ in $$\mathbb{R}^K$$, a Gaussian process $$f: \mathbf{x} \rightarrow \mathbb{R} \sim \mathcal{GP}(m, K)$$ satisfies

$$\mathbf{f} = (f(\mathbf{x}_1), \dots, f(\mathbf{x}_N)) \sim \mathcal{N}(\boldsymbol \mu, \boldsymbol \Sigma) \label{eq:gp}$$

$$\mu_i = m(\mathbf{x}_i)$$

$$\Sigma_{i,j} = K(\mathbf{x}_i, \mathbf{x}_j)$$

The idea is that the Gaussian process acts as a prior over the *function* $$f$$. However, it does not actually tell us how to sample $$f$$ in its entirety. Instead, it tells us show to sample $$f$$ *point-wise*. In particular, any arbitrary sample of points has a normal distribution. This a potentially high-dimensional distribution (if we sample $$f$$ at many points). The key to the game is the assumption that the covariance is a function of $$\mathbf{x}$$. Useful choices of kernel $$K$$ will result in high covariance between values of $$f$$ sampled at points $$\mathbf{x}_i, \mathbf{x}_j$$ that are close together, which results in a smoothly varying function. (How smooth is determined by parameters of the kernel function).

A typical choice of kernel function is the *radial basis function*,

$$
\begin{equation}
    \kappa(\mathbf{x}, \mathbf{x}^\prime) = \exp \left( -\frac{1}{2} (\mathbf{x} - \mathbf{x}^\prime)^{\top} \boldsymbol \Sigma^{-1}(\mathbf{x} - \mathbf{x}^\prime)  \right)
\end{equation}
$$

For a fixed $$\mathbf{x}^\prime$$, the function describes a normal distribution centered at $$\mathbf{x}^\prime$$ with variance $$\boldsymbol \Sigma$$. We are free to choose any kernel we wish so long as it results in positive-definite covariance matrix in \eqref{eq:gp}.

How do we use these things? Gaussian processes are used for non-parametric regression. (They are non-parametric because we don't specify a parametric form for the approxmation, only for the kernel function). They are typically used for spatial, temporal, or spatio-temporal regressions: think of them as a method for curve fitting without specifying the form of the curve. In some cases the function $$f$$ is the primary object of interest; in other cases, $$f$$ plays a supporting role in a probabilistic model.

**Example** A canonical example of Gaussian processes is given by the following regression problem. Suppose that we observe a time series $$y_i, \dots, y_N$$ at corresponding times $$x_i, \dots, x_N$$. We imagine that $$\mathbf{y}$$ is a noisy observation of the process of interest, $$f(x)$$, which we model as a Gaussian process. Thus, our model is

$$
\begin{align}
    f &\sim \mathcal{GP}(0, K) \\
    y \ | \ f &\sim \mathcal{N}(f, \Sigma)
\end{align}
$$

We wish to learn about $$f$$, so we do as any good Bayesian and consider it's posterior distribution:

$$
\begin{align}
    f \ | \ y \propto \mathcal{N}(f, \Sigma) \times \mathcal{GP}(0, K)
\end{align}
$$

The problem here is that we have no idea what the righthand side of this equation even means: what is a normal distribution centered at the *function* $$f$$, and how do I sample from $$\mathcal{GP}(0, K)$$? What we need to do instead is to consider a sample of this function, $$\mathbf{f} = (f_1, \dots, f_N)$$, taken at the same times that we observe $$\mathbf{y}$$. Our assumption that $$f$$ is generated by a Gaussian process tells that any such sample is normally distributed. Now we can write the model as:

$$
\begin{align}
    \mathbf{f} &\sim \mathcal{N}(0, \Sigma_f) \\
    \mathbf{y} \ | \ \mathbf{f} &\sim \mathcal{N}(\mathbf{f}, \Sigma_y)
\end{align}
$$

which is a simple linear Gaussian system whose posterior is given by

$$
\begin{align}
    p(\mathbf{f} \ | \ \mathbf{y}) &= \mathcal{N}(\mathbf{f} \ | \ \boldsymbol \mu_{f | y}, \boldsymbol \Sigma_{f | y}) \\
    \boldsymbol \Sigma_{f | y} &= \boldsymbol \Sigma_f^{-1} + \boldsymbol \Sigma_f^{-1} \\
    \boldsymbol \mu_{f | y} &= \boldsymbol \Sigma_{f | y} \left[ \boldsymbol \Sigma_y^{-1} \mathbf{y} \right]
\end{align}
$$

(This is a well-known result for linear Gaussian systems). In summary, regression using Gaussian processes amounts to assuming a linear Gaussian system between the latent process $$\mathbf{f}$$ and the observed process $$\mathbf{y}$$, where the distribution of the former is determined by a the choice of kernel function $$K$$.

**Example** As a more advanced example, consider the so-called log Gaussian Cox process (LGCP). This process is used to model the arrival of events whose likelihood varies over time, i.e., it is an inhomogeneous Poisson process. Specifically, we assume that the arrival of events follows as Poisson process with intensity given by a Gaussian process:

$$
\begin{align}
    f &\sim \mathcal{GP}(0, K) \\
    \lambda(t) &= \exp(f(t)) \\
    p(y \ | \ \lambda(t)) &= \mathcal{PP}(y \ | \ \lambda(t))
\end{align}
$$

Again, we will not infer $$f$$ directly, but instead consider a vector $$\mathbf{f}$$ of observations at points $$x_1, \dots, x_N$$. The posterior of this vector is given by

$$
\begin{align}
    p(\mathbf{f} \ | \ y) \propto \mathcal{PP}(y \ | \ \boldsymbol \lambda) \times \mathcal{N}(\mathbf{f} \ | \ 0, \boldsymbol \Sigma)
\end{align}
$$

In this case, we have no idea what this distribution looks like. Thus, we will need to rely on sampling methods to approximate it. [Neal, 2003](https://projecteuclid.org/journals/annals-of-statistics/volume-31/issue-3/Slice-sampling/10.1214/aos/1056562461.full) proposed a Metropolis-Hastings algorithm for sampling such distributions. The proposal distribution is given by

$$
\begin{equation}
    \mathbf{f}^\prime = \sqrt{1 - \epsilon^2} \ \mathbf{f} + \epsilon \boldsymbol \nu, \ \boldsymbol \nu \sim \mathcal{N}(0, \boldsymbol \Sigma)
\end{equation}
$$

with acceptance probability

$$
\begin{equation}
    p(\text{accept}) = \min \left(1, \frac{p(y \ | \ \mathbf{f^\prime})}{p(y \ | \ \mathbf{f})} \right)
\end{equation}
$$

The proposal is a mixture of the current sample and a draw from the Gaussian process prior, which is always accepted if $$\mathbf{f}^\prime$$ has a higher likelihood than $$\mathbf{f}$$. [Murray, 2010](http://proceedings.mlr.press/v9/murray10a/murray10a.pdf) introduced an alternative method, ellpitical slice sampling, which is generally more efficient and eliminates the need to select the step size, $$\epsilon$$.

## References
[Neal, 2003](https://projecteuclid.org/journals/annals-of-statistics/volume-31/issue-3/Slice-sampling/10.1214/aos/1056562461.full)  R. M. Neal. *Slice sampling*. Annals of Statistics, 31(3):705–767, 2003.

[Murray, 2010](http://proceedings.mlr.press/v9/murray10a/murray10a.pdf) Iain Murray, Ryan Adams, and David MacKay.  Elliptical slice sampling.  In *Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics*, volume 9, pages 541–548. PMLR, 13–15 May 2010. 