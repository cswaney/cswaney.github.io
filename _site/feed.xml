<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-10-04T07:43:45-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">katabaticwind</title><subtitle>A research and development blog.</subtitle><entry><title type="html">The Linked List Cycle Problem</title><link href="http://localhost:4000/programming/2019/09/17/the-linked-list-cycle-problem.html" rel="alternate" type="text/html" title="The Linked List Cycle Problem" /><published>2019-09-17T00:00:00-04:00</published><updated>2019-09-17T00:00:00-04:00</updated><id>http://localhost:4000/programming/2019/09/17/the-linked-list-cycle-problem</id><content type="html" xml:base="http://localhost:4000/programming/2019/09/17/the-linked-list-cycle-problem.html">&lt;p&gt;I’ve been studying basic computer science concepts recently because I don’t have a “traditional” programming background, but people like to ask these types of questions at interviews for some reason (I don’t exactly understand why). In any case, some of the practice problems are kind of fun, like the one I’m about to describe concerning linked lists. First, what is a linked list? It’s merely a collection of objects (usually called “nodes”) in which each object holds a value and a pointer to one of the other objects. Linked lists are useful for implementing stacks and queues because it is easy to remove objects from the beginning or end of a linked list. (On the other hand, they aren’t too useful as iterable objects due to the way the memory of the nodes is laid out). However, a linked list could contain a cycle in which the “last” object points back to an object that came before it. The question is simply how to detect whether a linked list contains such a cycle assuming that you’ve been told which node is “first” in the list.&lt;/p&gt;

&lt;p&gt;There is an obvious answer to this question: iterate through the linked list, keeping track of each node you see in a dictionary and return &lt;code class=&quot;highlighter-rouge&quot;&gt;true&lt;/code&gt; if you find a node that points back to a node in your dictionary. This solution is &lt;script type=&quot;math/tex&quot;&gt;\mathcal{O}(n)&lt;/script&gt; in time and space. It turns out, however, that there is a simple solution that reduces the space complexity to &lt;script type=&quot;math/tex&quot;&gt;\mathcal{O}(1)&lt;/script&gt;, which I’ll now explain.&lt;/p&gt;

&lt;p&gt;Let’s imagine two people “walking” through the linked list. The first person walks slowly, moving one node each step; the second person walks twice as fast (two nodes per iteration). We start our walkers out at the first node and pause each step to check each walker’s position. Now our first claim is that the walkers will meet at some point in time if and only if the linked list contains a cycle. Intuitively, if the walkers enter a cycle, we know that the faster walker will eventually catch up with the slower walker. Otherwise, the speedier walker will always remain ahead of his leisurely partner, so this method will return &lt;code class=&quot;highlighter-rouge&quot;&gt;true&lt;/code&gt; if and only the linked list contains a cycle. But how can we be sure that the faster walker will catch the slower walker in &lt;script type=&quot;math/tex&quot;&gt;\mathcal{O}(n)&lt;/script&gt; time?&lt;/p&gt;

&lt;p&gt;In fact, we can calculate &lt;em&gt;exactly&lt;/em&gt; when the walkers will meet. The reason boils down to the following observation: a person starting &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; steps behind but walking twice as fast as another person will catch the slower person in precisely &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; steps! Now consider the moment when our slow walker has just arrived at the beginning of a cycle that contains &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; nodes. Wherever our fast walker is, we can be sure that it is at most &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; nodes ahead. In terms of catching up to the slow walker, that means the fast walker is effectively at most &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; nodes &lt;em&gt;behind&lt;/em&gt;. But that means that she will catch the slow walker in &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; steps. So not only will our walkers cross, but they will in fact land on the same node after some number of steps, and that will always happen within at worst &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; iterations.&lt;/p&gt;</content><author><name>Colin Swaney</name></author><category term="programming" /><category term="programming" /><summary type="html">I've been studying basic computer science concepts recently because I don't have a traditional programming background, but people like to ask these types of questions at interviews for some reason (I don't exactly understand why). In any case, some of the practice problems are kind of fun, like this one concerning linked lists.</summary></entry><entry><title type="html">Policies and Optimality</title><link href="http://localhost:4000/research/2019/07/15/policies-and-optimality.html" rel="alternate" type="text/html" title="Policies and Optimality" /><published>2019-07-15T00:00:00-04:00</published><updated>2019-07-15T00:00:00-04:00</updated><id>http://localhost:4000/research/2019/07/15/policies-and-optimality</id><content type="html" xml:base="http://localhost:4000/research/2019/07/15/policies-and-optimality.html">&lt;p&gt;In the last lecture I introduced the general setting of deep reinforcement learning, which consists of computational agents interacting with a digital environment, earning rewards, and observing how its actions affect the future state of the world. Formally, this system is described as a Markov Decision Process. In this lecture I want to discuss how agents interact with their environment, describe how we evaluate the fitness of agents’ strategies, and finish by defining what is meant by “optimality” in reinforcement learning.&lt;/p&gt;

&lt;!-- The goal of reinforcement learning is to train computers to perform tasks *well*. Ideally, algorithms lead to *optimal* decision-making. Let's take a moment to discuss a number of important ideas that will show up everywhere in the study of deep reinforcement, which are motivated by dynamic programming: the study of optimal planning in dynamic (unfolding over time) processes. --&gt;

&lt;h2 id=&quot;policies&quot;&gt;Policies&lt;/h2&gt;
&lt;p&gt;An agent’s actions are determined by a &lt;em&gt;policy&lt;/em&gt;, which specifies the probability of every possible action that our agent can take in each state of the world. For simplicity, let’s assume that our world is finite so that we can enumerate the states of the world &lt;script type=&quot;math/tex&quot;&gt;s_t \in \{0, 1, \dots, K - 1\} = \mathcal{S}&lt;/script&gt; as well as the possible actions &lt;script type=&quot;math/tex&quot;&gt;a_t \in \{0, 1, \dots, N\} = \mathcal{A}&lt;/script&gt;. For each action &lt;script type=&quot;math/tex&quot;&gt;a \in \mathcal{A}&lt;/script&gt; and each state &lt;script type=&quot;math/tex&quot;&gt;s \in \mathcal{S}&lt;/script&gt;, a policy defines the probability our agent performs the action at time step &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi(a \ \vert \ s) = p(a_t = a\ \vert \ s_t = s)&lt;/script&gt;

&lt;p&gt;The policy defines a proper probability distribution, so we can use it to compute statistical quantities such as the expected reward at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}_{\pi} \left[ r_t \ \vert \ s_t = s \right] = \sum_{a} \pi(a \ \vert \ s) \sum_{r_t, \ s_{t + 1}} p(r_t, s_{t + 1} \ \vert \ s_t = s, a_t = a) \ r_t&lt;/script&gt;

&lt;p&gt;(The notation &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}_{\pi}[x]&lt;/script&gt; is shorthand for &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E} \left[ x \ \vert \ a_t \sim \pi(a \ \vert \ s) \right]&lt;/script&gt;. More generally, it means that &lt;em&gt;every&lt;/em&gt; action is chosen according to &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;). If we follow a deterministic policy, then &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; collapses and we can instead think of it as a mapping from states to actions, &lt;script type=&quot;math/tex&quot;&gt;\pi: \mathcal{S} \rightarrow \mathcal{A}&lt;/script&gt;. In that case, the above expectation simplifies to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}_{\pi} \left[ r_t \ \vert \ s_t = s \right] = \sum_{r_t, \ s_{t + 1}} p(r_t, s_{t + 1} \ \vert \ s_t = s, a_t = \pi(s_t)) \ r_t&lt;/script&gt;

&lt;p&gt;Our goal in reinforcement learning is learn &lt;em&gt;optimal&lt;/em&gt; policies. You may be wondering why our policy doesn’t take time into account—why don’t we need to specify what action to perform at each time step? It turns out that framing the problem setting as a Markov Decision Process means that we don’t need to think in terms of policies that span multiple time steps: we only need to determine the best action to perform in each state of the world. Such policies can lead to sophisticated strategies because agents learn to move from “bad” states to “good” states.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/breakout.gif&quot; alt=&quot;Breakout&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Consider the following concrete example. Suppose we are training an agent to play the classic Atari game, Breakout. The optimal strategy in Breakout is essentially to make a hole along one side of the bricks, then hit the ball through the hole so that it bounces around endlessly on top of the bricks. As we normally describe it, the strategies seems to be intimately connected to time: first we do one thing (make the hole), then we do another (hit the ball through the hole). But we can equally describe this strategy in terms of states: if the world doesn’t have a hole, perform actions that make a hole, and if the world has a hole, perform actions that lead to the ball going through the hole. So really we just need to know which states we should move towards from whatever state we find ourselves in. This is how the reinforcement agent views the world.&lt;/p&gt;

&lt;h2 id=&quot;values&quot;&gt;Values&lt;/h2&gt;
&lt;p&gt;There are two important functions associated with every policy that we use to evaluate fitness, and which form the basis of reinforcement learning algorithms. The &lt;em&gt;value&lt;/em&gt; of a policy is the expected return (sum of discounted rewards) “under the policy”,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V^{\pi}(s) = \mathbb{E}_{\pi} \left[ R_t \ \vert \ s_t = s \right] = \mathbb{E}_{\pi} \left[ \sum_{\tau=t}^{T} \gamma^{\tau - t} r_{\tau} \ \vert \ s_t = s \right],&lt;/script&gt;

&lt;p&gt;where the notation &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}_{\pi}\left[ \dots \right]&lt;/script&gt; means that all actions are performed with the probabilities specificed by &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;. Notice that the value of a policy depends on the current state: a policy might work well in one state of the world (high value), and poorly in another (low value). The goal of reinforcement learning is to choose the policy that has the highest value in &lt;em&gt;all&lt;/em&gt; states of the world—more on this in a moment.&lt;/p&gt;

&lt;p&gt;In addition to the value function, reinforcement learning relies heavily on the related concept known as an &lt;em&gt;action-value&lt;/em&gt; function (also commonly referred to as the “Q-function”). The action-value function of a policy is its expected return assuming we perform an arbitrary action &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; at time step &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; and then follow the policy,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q^{\pi}(s, a) = \mathbb{E}_{\pi} \left[ R_t \ \vert \ s_t = s, a_t = a \right].&lt;/script&gt;

&lt;p&gt;The only difference between the action-value function and the value function is the initial action performed: the value function follows the policy, while the action-value deviates from the policy. Action-value functions are useful because they allow us to ask the “what if” question (“What if I performed action &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; instead of action &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;?”), which is obviously a useful question to consider if we’d like to improve our policy!&lt;/p&gt;

&lt;p&gt;It’s worth pointing out (and you should convince yourself) that there is a simple relationship between the value and the action-value function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V^{\pi}(s) = \mathbb{E}_{\pi} \left[ Q^{\pi}(s, a) \right],&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q^{\pi}(s, a) = \mathbb{E}_{\pi} \left[ r_t + \gamma V^{\pi}(s_{t + 1}) \ \vert \ s_t = s, a_t = a \right].&lt;/script&gt;

&lt;h3 id=&quot;optimality--the-bellman-equation&quot;&gt;Optimality &amp;amp; The Bellman Equation&lt;/h3&gt;
&lt;p&gt;Clearly this value function is important—in some sense it is the only thing that matters (it’s what we’re trying optimize after all). But how do I calculate the value of a policy? Let’s consider a simplified problem where we can directly compute the value of a policy. So imagine that we are faced with a sequence of heads-or-tails decisions and that reward we receive depends in some arbitrary way on the sequence of coin flips we’ve seen so far. The situation is represented by a binary tree, where the value at each node represents the reward associated with the corresponding sequence of coin flips, as depicted below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/bellman/bellman.png&quot; alt=&quot;Bellman&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As shown, we flip the coin four times before the game ends. How do we decide what path to follow? The first step is to ask what the expected value of the game is after the second to last coin flip (the last columns of red nodes). The game ends on the next step no matter what, meaning that all states lead to the terminal state with no reward. So the expected reward at each node is the reward received at that node plus zero.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/bellman/bellman_2.png&quot; alt=&quot;Bellman&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now we repeat this analysis for the states following the second coin flip. We’ll fill in each of the expectations we just calculated (the new green nodes), and we can now ignore the terminal state. Our goal is to calculate the expected return from each node, which (if we are dealing with a fair coin) is equal to its reward plus the average of its childrens values. For example, the top node earns a reward of one, and the average of its children is one-half, so that node is overwritten by one and one-half.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/bellman/bellman_3.png&quot; alt=&quot;Bellman&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We keep repeating this process until we arrive at the root node, at which point we have calculated the expected value of the policy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/bellman/bellman_4.png&quot; alt=&quot;Bellman&quot; /&gt;
&lt;img src=&quot;/assets/img/bellman/bellman_5.png&quot; alt=&quot;Bellman&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This example motivates a straightforward method to compute the value of the policy: start from the end and work backwards. At the end of the game—at time &lt;script type=&quot;math/tex&quot;&gt;T+1&lt;/script&gt;—the value is zero, so we can write &lt;script type=&quot;math/tex&quot;&gt;V_{T + 1} = 0&lt;/script&gt;. Now we take one step back and ask what the value is going forward. At time &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; we earn a random reward &lt;script type=&quot;math/tex&quot;&gt;r_T&lt;/script&gt;, then transtion to &lt;script type=&quot;math/tex&quot;&gt;s_{\text{end}}&lt;/script&gt; and earn zero additional reward, so the value is &lt;script type=&quot;math/tex&quot;&gt;V_T = \mathbb{E}_{\pi}\left[ r_T \right]&lt;/script&gt;. Let’s continue to move backward in time: at time &lt;script type=&quot;math/tex&quot;&gt;T - 1&lt;/script&gt; we earn a random reward &lt;script type=&quot;math/tex&quot;&gt;r_{T-1}&lt;/script&gt;, then we transition to state &lt;script type=&quot;math/tex&quot;&gt;s_T&lt;/script&gt; where we already know that we will earn a value of &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}_{\pi}\left[ r_{T} \right]&lt;/script&gt; going forward. Therefore the value at time &lt;script type=&quot;math/tex&quot;&gt;T-1&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}_{\pi} \left[ r_{T-1} + \mathbb{E}_{\pi} \left[ r_T \right] \right] = \mathbb{E}_{\pi} \left[ r_{T - 1} + V_T \right].&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;If we continue moving backwards in this fashion we’ll see that every step along the way the value at step &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; is always equal to &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}_{\pi} \left[ r_{t} + V_{t + 1} \right],&lt;/script&gt; which we can always compute because we started by saying that &lt;script type=&quot;math/tex&quot;&gt;V_{T+1} = 0&lt;/script&gt;. Thus, we have figured out a simple algorithm to compute the value of our random policy. The key was recursion: relate the value today to the value tomorrow. In this example we didn’t worry about states of the world, but essentially the same logic works in the Markov Decision Process setting of reinforcement learning. The &lt;em&gt;Bellman equation&lt;/em&gt; demonstrates that the value of a given policy satisfies a particular recursive property:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V^{\pi}(s) = \mathbb{E}_{\pi} \left[r_t + \gamma V^{\pi}(s_{t + 1}) \ \vert \ s_t = s \right]&lt;/script&gt;

&lt;p&gt;In words, the expected value of following a policy can be broken into the expected reward that period and the expected &lt;em&gt;return&lt;/em&gt; from all subsequent periods, and the later is simply the value one period in the future. We need to take the expectation of the future value because the future state is random, and where we end up next period depends on the action we choose today through the transition probability &lt;script type=&quot;math/tex&quot;&gt;p(r_{t + 1}, s_{t + 1} \ \vert \ s_t, a_t)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The Bellman equation is interesting because it defines &lt;script type=&quot;math/tex&quot;&gt;V^{\pi}&lt;/script&gt; in the sense that the value function of &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; is its unique solution. Just as in the example above, the Bellman equation tells us how to compute the value of a policy. In fact, if there are a finite number of states—let’s say &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; to be concrete—then the Bellman equation is really a &lt;script type=&quot;math/tex&quot;&gt;K \times K&lt;/script&gt; system of equations, and &lt;script type=&quot;math/tex&quot;&gt;V^{\pi} \in \mathbb{R}^K&lt;/script&gt; is its solution, which you can perhaps see this more clearly by writing out the Bellman equation using probabilities:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V^{\pi}(s) = \sum_a \pi(a \ \vert \ s) \sum_{r_t, \ s_{t + 1}} p(r_t, s_{t + 1} \ \vert \ s_t = s, a_t = a) \left[ r_t + \gamma V^{\pi}(s_{t + 1}) \right]&lt;/script&gt;

&lt;p&gt;This equation holds for every &lt;script type=&quot;math/tex&quot;&gt;s \in \mathcal{S}&lt;/script&gt;, and so for each state we get an equation that involves known probabilities (&lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;) and the value in each state &lt;script type=&quot;math/tex&quot;&gt;s_{t + 1} \in \mathcal{S}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Now the goal of reinforcement learning is to learn &lt;em&gt;optimal&lt;/em&gt; policies (or approximately optimal policies). So how do we define an optimal policy? A policy is optimal if its value is at least as large as any other policy in every state of the world. It’s common to denote an optimal policy (which may not be unique) by &lt;script type=&quot;math/tex&quot;&gt;\pi^{\ast}&lt;/script&gt;, and to denote the corresponding value and action-value functions by &lt;script type=&quot;math/tex&quot;&gt;V^{\ast}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Q^{\ast}&lt;/script&gt;. While the optimal policy may not be unique, the optimal value function &lt;em&gt;is&lt;/em&gt;, with the practical implication that it isn’t necessary to directly look for the optimal policy. Instead, we can look for the optimal value, and then define an optimal policy based on the optimal value function (more of this in the following lecture).&lt;/p&gt;

&lt;p&gt;Whatever the optimal value is, it must adhere to the Bellman equation (it is the result of &lt;em&gt;some&lt;/em&gt; policy after all). However, the optimality of the policy allows us to write this equation out slightly differently:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V^{\ast}(s) = \max_a \mathbb{E}_{\pi^\ast} \left[ r_t + \gamma V^{\ast}(s_{t + 1}) \ \vert \ s_t = s, a_t = a \right]&lt;/script&gt;

&lt;p&gt;Writing out the “&lt;script type=&quot;math/tex&quot;&gt;\max_a&lt;/script&gt;” doesn’t really change this expression from what we wrote down before because (by definition) following &lt;script type=&quot;math/tex&quot;&gt;\pi^{\ast}&lt;/script&gt; already implies that we take a maximizing action, but the notation makes it explicit that we only need to worry about what happens from this period to the next. The Bellman equation is at the core of many deep reinforcement learning algorithms. In my next post, I’ll take a look at the role it plays in some &lt;em&gt;classical&lt;/em&gt; reinforcement learning algorithms.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;[SB] Sutton &amp;amp; Barro,. &lt;a href=&quot;https://mitpress.mit.edu/books/reinforcement-learning-second-edition&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;, (2018).&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Colin Swaney</name></author><category term="research" /><category term="reinforcement learning" /><summary type="html">In this mini-lecture I continue looking at some of the basic concepts underlying deep reinforcement learning and control. I'll define what is meant by a &quot;policy&quot; and how we might characterize the value of a policy. Finally, I'll discuss Bellman's optimality condition, which lies at the heart of the dynamic optimization problems we hope to solve in deep reinforcement learning.</summary></entry><entry><title type="html">Introduction to Deep Reinforcement Learning</title><link href="http://localhost:4000/research/2019/07/08/deep-reinforcement-learning-introductioin.html" rel="alternate" type="text/html" title="Introduction to Deep Reinforcement Learning" /><published>2019-07-08T00:00:00-04:00</published><updated>2019-07-08T00:00:00-04:00</updated><id>http://localhost:4000/research/2019/07/08/deep-reinforcement-learning-introductioin</id><content type="html" xml:base="http://localhost:4000/research/2019/07/08/deep-reinforcement-learning-introductioin.html">&lt;p&gt;This is the first in a series of introductory lectures on deep reinforcement learning. I plan to follow the classic text by Sutton and Barro [SB], but I will also borrow from the lecture notes used in Sergey Levine’s &lt;a href=&quot;http://rail.eecs.berkeley.edu/deeprlcourse/&quot;&gt;course on deep reinforcement learning&lt;/a&gt; taught at Berkeley for the last few years. We won’t get into the “deep” part of “deep reinforcement learning” for a few lectures, but hopefully laying out some groundwork will make the more modern ideas to come more meaningful.&lt;/p&gt;

&lt;h2 id=&quot;the-goal-of-reinforcement-learning&quot;&gt;The Goal of Reinforcement Learning&lt;/h2&gt;
&lt;p&gt;The goal of reinforcement learning is to train computers to perform complex, dynamic tasks, which generally involve &lt;em&gt;planning&lt;/em&gt;. From a mathematical perspective, reinforcement learning presents itself as a computational—at times almost heuristic—alternative to traditional control theory. Both fields offer solutions to the general problem of determining an optimal course of action in a dynamic, and possibly uncertain, environment, which I refer to as the “control problem”. Control problems are &lt;em&gt;complicated&lt;/em&gt;, but we can break them down into two parts. First, there is a time series or stochastic processes component, which reflects how the system changes over time as well as how the system reacts to actions performed by the agent. Second, there is an optimization component, which involves determining how to act to alter—or control—these dynamics so that the system achieves some desired outcome, or follows some optimal trajectory.&lt;/p&gt;

&lt;p&gt;Traditional mathematical approaches to these types of problems represent some of the crowning achievements of applied mathematics. In some cases, they provide &lt;em&gt;exact&lt;/em&gt; formulae describing what actions to take and when; for a surprisingly board range of problems, they prove that certain numerical methods generate approximate solutions that can be made arbitrarily close a ground truth solution. To give just one example, the “genius” of modern finance—the “astrophysics” that is derivatives pricing—is nothing more than applied stochastic control. But for all the beauty of mathematical optimal control, it doesn’t have the flexibility required to solve many apparently simple real-world problems. The real world is messy, and it is in that messy world that reinforcement learning takes center stage.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/control.png&quot; alt=&quot;The control problem&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;agents--environments&quot;&gt;Agents &amp;amp; Environments&lt;/h2&gt;
&lt;p&gt;Reinforcement learning revolves around &lt;em&gt;agents&lt;/em&gt; and &lt;em&gt;environments&lt;/em&gt;. An agent is a computer that makes decisions and selects actions to perform; an environment is the world that the agent observes and interacts with. Reinforcement learning generally considers sequences of decisions made over time, and problems in which the actions an agent selects at one time step affect the environment the agent observes in the following step. In principle, these are just optimization problems, but they are more complicated than “standard” optimization problems because they require the agent to determine the optimal action to perform in every possible state of the world.&lt;/p&gt;

&lt;p&gt;Unlike classic mathematical approaches, reinforcement learning algorithms involve &lt;em&gt;interaction&lt;/em&gt; between the agent and the environment. Agents learn about the environment by performing actions and seeing how the environment evolves, as depicted in the figure above. At each point in time, the agent looks at the current state of the world (or her perception of it at least), selects an action to perform, and then observes both a reward as well as the next state of the world. In other words, we design reinforcement learning agents to learn through experimentation, through trial-and-error.&lt;/p&gt;

&lt;p&gt;One of the fundamental problems that arises in reinforcement learning is the trade-off between &lt;em&gt;exploitation&lt;/em&gt; and &lt;em&gt;exploration&lt;/em&gt;. Whenever an agent performs an action, it affects both the reward received and the next state of the world. One the hand, the agent wants to select actions that it believes generate higher rewards (because its trying to maximize rewards). On the other hand, if it only chooses actions that it currently believes to generate higher rewards, it might never learn other actions that lead to states of the world where she can generate even higher rewards. It is a situation that we are all familiar with: do we go with the best option that we &lt;em&gt;know&lt;/em&gt;, or do we take a chance and try something new? Our choice will determine our information set (we’ll never know if we should have taken that other job offer), and therefore what we can learn—and likewise for our agent.&lt;/p&gt;

&lt;p&gt;I’ve been talking about “time” and “states”. It’s worth pointing out that “time” in this context doesn’t mean “natural time”: it’s just an ordering of events, which can have an arbitrary amount of (natural) time between them. For example, time might count each occurrence of a user visiting a web application, in which case the steps will be spaced out randomly. Alternatively, we might be thinking about an agent that checks the state of the world on a fixed schedule (e.g. once per second). The goal of reinforcement learning is not to model or predict the timing of events, but to determine how to respond to events as they occur. Likewise, “state” should not be construed to mean a “natural state” of the world, but can be thought of abstractly. State &lt;em&gt;can&lt;/em&gt; mean measurable quantities like location and speed, but it can also refer to a “state of mind” or a “state of thought” [SB].&lt;/p&gt;

&lt;h3 id=&quot;dynamics&quot;&gt;Dynamics&lt;/h3&gt;
&lt;p&gt;The theory of reinforcement learning centers on a simple but flexible assumption regarding the way the environment works: we assume that whatever happens each time step depends only on the current state of the world and on the agent’s present choice of action. The rest of history is &lt;em&gt;irrelevant&lt;/em&gt; to what happens this time step. In other words, it doesn’t matter &lt;em&gt;how&lt;/em&gt; we arrived at a state, it only matters that we are in that state.&lt;/p&gt;

&lt;p&gt;Mathematically, we call the system a &lt;em&gt;Markov decision process&lt;/em&gt; (MDP). Let’s denote the state of world at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; by &lt;script type=&quot;math/tex&quot;&gt;s_t&lt;/script&gt;, our agent’s action by &lt;script type=&quot;math/tex&quot;&gt;a_t&lt;/script&gt;, the rewards received by &lt;script type=&quot;math/tex&quot;&gt;r_t&lt;/script&gt;, and the new state &lt;script type=&quot;math/tex&quot;&gt;s_{t + 1}&lt;/script&gt;. The Markov assumption says that &lt;script type=&quot;math/tex&quot;&gt;r_t&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;s_{t + 1}&lt;/script&gt; are determined by some probability distribution represented by some joint probability density&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(r_t, s_{t + 1} \ \vert \ a_t, s_t),&lt;/script&gt;

&lt;p&gt;which completely describes the dynamics of the system—&lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; is &lt;em&gt;all&lt;/em&gt; we need to know in order to understand how the environment works. But we haven’t said anything about the nature of &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; other than that it is a valid probability distribution. In many applications, &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; is in fact a deterministic—but extremely complicated—function. Nonetheless, the assumption means that &lt;em&gt;if&lt;/em&gt; it knows &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;, then our agent can make optimal decisions based on the current state of the world, and nothing else.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/mdp.png&quot; alt=&quot;mdp&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The figure above shows a canonical representation of the MDP. If you’ve studied recurrent neural networks, this diagram ought to look familiar—at a high level it’s the same as a vanilla RNN. Alternatively, you might see this figure as a special form of hidden Markov model in which we add an additional “forcing” variable &lt;script type=&quot;math/tex&quot;&gt;a_t&lt;/script&gt; to the mix. Either way, the Markov assumption reduces the problem of understanding the dynamics of the system to that of learning a single distribution represented by &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;. In the case of a hidden Markov model, &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; is typically a separable function taking the form &lt;script type=&quot;math/tex&quot;&gt;p(r, s' \ \vert \ s, a) = f(r \ \vert \ s, a) \ g(s' \ \vert \ s, a)&lt;/script&gt;. In the case of vanilla RNN, &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; is also separable, but &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; are represented by hidden layers instead of explicit probability distributions. In reinforcement learning we make no such assumptions about &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;. As you will see, reinforcement learning often avoids learning about &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; directly.&lt;/p&gt;

&lt;p&gt;It might seem to you that the Markov assumption is too restrictive: surely many interesting problems require us to know more than just the most recent history! In practice this isn’t really an issue because we are always free to redefine what we mean by “state”. For example, if you tell me that tomorrow’s weather depends on the last two days of weather, and not just yesterday’s weather, then I will simply change my definition of state from “the last day’s weather” to “the last &lt;em&gt;two&lt;/em&gt; days’ weather”. As [SB] puts it, “This is best viewed a [sic] restriction not on the decision process, but on the state.” In other words, any process that we have a reasonable hope of controlling &lt;em&gt;will&lt;/em&gt; satisfy the Markov property for &lt;em&gt;some&lt;/em&gt; definition of the state.&lt;/p&gt;

&lt;h3 id=&quot;rewards&quot;&gt;Rewards&lt;/h3&gt;
&lt;p&gt;Agents learn optimal plans via reward signals designed and specified by us, their human overlords. Rewards can be understood by analogy with neurotransmitters (e.g. dopamine), which signal the desirability of different outcomes to the brain. Such neurotransmitters motivate us to perform actions that lead to (or avoid) particular states of the world through experience. In the case of reinforcement learning, researchers specify the amount of reward associated with the environment, and rewards are therefore arbitrary in some sense. On the other hand, the performance of a reinforcement learning algorithm is highly dependent on the choice of rewards, with poorly chosen rewards resulting in agents that fail to learn desired outcomes, or grow “addicted” to sub-optimal behaviors.&lt;/p&gt;

&lt;p&gt;Fortunately, a natural choice for rewards will often work. For example, an agent trained to play a two-player video game might earn a reward of one for winning, and negative one for losing, or it might earn rewards proportional to its score in a one-person game. In other cases an appropriate reward system might be less obvious: what rewards should we specify for a robot learning to walk? The key to remember is that agents generally have little pre-programmed “knowledge”: unlike the natural world, they lack genetic predisposition towards particular behaviors and will engage in any manner of outrageous behavior if it elicits reward.&lt;/p&gt;

&lt;p&gt;From the agent’s perspective rewards are just some numerical value; reinforcement learning algorithms encourage the computer to select actions that generate greater amounts of reward. In the typical reinforcement learning setting the agent’s goal is to maximize the expected amount of total reward received:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max_{\{a_0, \dots, a_T\}} \mathbb{E} \left[ R \right] = \max_{\{a_0, \dots, a_T\}} \mathbb{E} \left[ \sum_{t = 0}^T \gamma^t r_t \right].&lt;/script&gt;

&lt;p&gt;We refer to the quantity &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt; as the &lt;em&gt;return&lt;/em&gt;, and the expectation is taken with respect to the transition probability &lt;script type=&quot;math/tex&quot;&gt;p(r_t, s_{t + 1} \ \vert \ a_t, s_t)&lt;/script&gt;. The quantity &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; is called the “discount factor”, and it imparts the agent with a time-preference over rewards: when &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\gamma &lt; 1 %]]&gt;&lt;/script&gt;, the agent prefers to receive rewards early rather than later, all else being equal.&lt;/p&gt;

&lt;p&gt;It’s important to recognize that there is no aspect of &lt;em&gt;risk&lt;/em&gt; in the above objective. From a behavioral perspective, its clear that individuals’ actions will generally reflect both risk and reward. For this reason economists typically assume that agents choose actions that maximize a so-called &lt;em&gt;utility function&lt;/em&gt;, &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;, which involves both the expectation and variance (or standard deviation) of rewards. For example, an investor might seek to maximize&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;U(R) = \frac{\mathbb{E}\left[ R \right]}{\text{var} \left[ R \right]},&lt;/script&gt;

&lt;p&gt;reflecting the intuitive notion that for any fixed expected return, a lower risk outcome is preferable. This is a interesting topic because adjusting for risk tolerance is likely essential to making deep reinforcement learning solutions palatable in many practical settings.&lt;/p&gt;

&lt;p&gt;Reward design deserves more attention that I can give it here, but I feel it’s important to reiterate [SB], who point out that rewards are not a good mechanism for encouraging solutions with certain properties. We don’t use rewards to encourage a robot to learn sub-tasks because the agent might learn to &lt;em&gt;only&lt;/em&gt; perform the sub-tasks, ignoring the outcome we actually care about! In other words, we want to focus agents on one goal, and let the agent figure out how to achieve that goal.&lt;/p&gt;

&lt;!-- We can pass prior knowledge to agents through their policy or initial value function, for example, by placing restrictions on the shape of the policy function or by removing certain actions from the choice of possible actions in states where we know they are sub-optimal. **I think it is sort of in general interesting to compare the policy of (deep) RL algorithms to the true optimal policy in problems where we can derive (or numerically approximate to arbitrary accuracy) the optimal solution.** --&gt;

&lt;h4 id=&quot;infinite-vs-finite-horizon&quot;&gt;Infinite vs Finite Horizon&lt;/h4&gt;
&lt;p&gt;Textbook presentations of reinforcement learning make a point of distinguishing between problems which eventually come to a finish—“finite horizon problems”—and problems that continue indefinitely with non-zero probability, so-called “infinite horizon problems”. The distinction has theoretical implications, but I view it as largely a distraction. One reason that the infinite horizon case is brought up is to motivate the idea of the discount factor mentioned above, which is optional in the finite horizon case, but  &lt;em&gt;required&lt;/em&gt; in the infinite horizon case to guarantee that &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt; is well-defined.&lt;/p&gt;

&lt;p&gt;As mentioned above, there is (potentially) a strong behavioral motivation behind discount factors in either case, but the most successful applications of deep reinforcement learning thus far have come in settings where time-preference over rewards plays an minor role (think &lt;script type=&quot;math/tex&quot;&gt;\gamma \approx 0.99&lt;/script&gt;). Nonetheless, if a reinforcement learning agent is used to learn an economic task, choosing an appropriate discount factor might be critical to finding an acceptable solution.&lt;/p&gt;

&lt;h3 id=&quot;examples&quot;&gt;Examples&lt;/h3&gt;
&lt;p&gt;Let’s take a brief look at three example environments. While admittedly contrived, these examples are useful for testing algorithms and simple to understand. Each of these examples is available in OpenAI’s &lt;a href=&quot;https://gym.openai.com&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;gym&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;

&lt;h4 id=&quot;cartpole&quot;&gt;CartPole&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/cartpole.gif&quot; alt=&quot;CartPole&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The pole-balancing problem is a classic in reinforcement learning. The environment consists of a cart that moves side-to-side along on a one-dimensional track, with a long pole attached to the cart via a hinge. When the cart is pushed one way, the pole falls in the opposite direction. We start the environment by placing the pole is some random “not-quite-vertical” position, and the goal of the agent is to keep the pole from falling over (or not letting the pole fall past a certain angle, after which it is impossible to bring the pole back upright).&lt;/p&gt;

&lt;p&gt;The only information that the agent needs to know is the position of the cart, &lt;script type=&quot;math/tex&quot;&gt;x_t&lt;/script&gt;, and the angle of the pole, &lt;script type=&quot;math/tex&quot;&gt;\theta_t&lt;/script&gt;. Therefore the state is two-dimensional: &lt;script type=&quot;math/tex&quot;&gt;s_t = (x_t, \theta_t).&lt;/script&gt; There are two available actions: “push left”, and “push right” (assume that the agent can only push with some fixed amount of force). We say that the state space is continuous, while the action space is discrete. You could make the problem more complicated by allowing the agent to choose the direction to push &lt;em&gt;and&lt;/em&gt; the amount of force to use. In that case, the action space would be mixed, the direction being discrete, and the force continuous.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;gym&lt;/code&gt; package’s implementation of the pole-balancing problem is called &lt;code class=&quot;highlighter-rouge&quot;&gt;CartPole&lt;/code&gt; (shown above). The environment is extremely simple to solve. We don’t need any fancy algorithms to solve it, but it is useful for debugging: if it doesn’t solve &lt;code class=&quot;highlighter-rouge&quot;&gt;CartPole&lt;/code&gt;, it doesn’t work. I also like to think of the inverse statement as being &lt;em&gt;almost&lt;/em&gt; true: if it solves &lt;code class=&quot;highlighter-rouge&quot;&gt;CartPole&lt;/code&gt;, it &lt;em&gt;probably&lt;/em&gt; works—so if you’re having problems with some other environment, maybe focus on errors in the environment-specific part of your code.&lt;/p&gt;

&lt;!-- ```python
import gym
env = gym.make('Pong-v4')
env = gym.wrappers.Monitor(env, '/Users/colinswaney/Desktop/pong/', force=True)
env.reset()
env.render()
steps = 0
while True:
    state, reward, done, info  = env.step(env.action_space.sample())
    steps += 1
    if done:
        print(steps)
        break
env.close()
```

```shell
mkdir frames
ffmpeg -i *.mp4 -r 5 'frames/frame-%03d.jpg'
convert -delay 20 -loop 0 ./frames/*.jpg pong.gif
``` --&gt;

&lt;h4 id=&quot;lunarlander&quot;&gt;LunarLander&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/lunar-lander.gif&quot; alt=&quot;LunarLander&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A classic problem in optimal control considers how to land a rocket on the moon. The goal is actually to land the rocket using the minimal amount of fuel (as otherwise you could always make a pillow-soft landing by burning through as much fuel as you want). Landing the rocket means that you arrive at a specific location (or within a specified region) with zero velocity and zero acceleration (and you don’t crash). In a standard version of this problem—which can be solved analytically [E]—the rocket drops straight down under the influence of gravity alone, and the pilot/onboard computer only needs to determine how much upward thrust to provide. In that case, the state space is continuous and one-dimensional (&lt;script type=&quot;math/tex&quot;&gt;y_t =&lt;/script&gt; height), as is the action space (&lt;script type=&quot;math/tex&quot;&gt;a_t =&lt;/script&gt; thrust).&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;gym&lt;/code&gt; version of this problem is a bit more interesting. First, the lander is dropped with a random orientation, so that every episode begins in a random state. Second, the lander is equipped with left-oriented and right-oriented thrusters in addition to a bottom thruster. The agent can only fire one of the thrusters (or none) at any instant, and the thrusters only have one setting (“full blast”). The state is described by eight variables:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;x_position&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;y_position&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;x_velocity&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;y_velocity&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;angle&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;angular_velocity&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;left_leg_contact&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;right_leg_contact&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first six variables are continuous, while the last two are discrete (they just indicate whether the legs are on the ground or not).&lt;/p&gt;

&lt;p&gt;Solving this environment is more difficult than solving the &lt;code class=&quot;highlighter-rouge&quot;&gt;CartPole&lt;/code&gt; environment, but still simple enough that it can serve as a useful testing ground for rapid prototyping of new algorithms. It doesn’t require any special handling of the observations, and if the algorithm works, it will converge relatively quickly.&lt;/p&gt;

&lt;h4 id=&quot;pong&quot;&gt;Pong&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/pong.gif&quot; alt=&quot;Pong&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dominating video games is the application of reinforcement learning that has received a lot of attention in the last few years. Pong is one of the easiest/fastest video games to learn. Reinforcement learning researchers use Atari games as benchmarks in research because they offer a wide range of tasks (from Pong to Seaquest), and because packages like OpenAi provide a standardized platform for training new algorithms against the full range of tasks. Computer agents learn to play Pong using essentially the same information that human players have access to: a “visual” representation of the game provided as a matrix of pixels. Agents are allowed perform any of the actions that a human can take. In Pong, this means “do nothing” (also referred to as “NOOP” for “No Operation”), “move paddle up”, or “move paddle down”.&lt;/p&gt;

&lt;p&gt;The following snippet of code demonstrates how to run a random policy on Pong for one episode with &lt;code class=&quot;highlighter-rouge&quot;&gt;gym&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;gym&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gym&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Pong-v4'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;info&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action_space&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This seems like a good place to stop for now. In the next lecture we’ll discuss policies and optimality.&lt;/p&gt;

&lt;!-- **Key terms**: reward, return, episode, action, state, Markov decision process (MDP) --&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;[SB] Sutton &amp;amp; Barro,. &lt;a href=&quot;https://mitpress.mit.edu/books/reinforcement-learning-second-edition&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;, (2018).&lt;/li&gt;
  &lt;li&gt;[E] Evans. &lt;a href=&quot;https://math.berkeley.edu/~evans/control.course.pdf&quot;&gt;“An Introduction to Mathematical Optimal Control theory”&lt;/a&gt;, (1983).&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Colin Swaney</name></author><category term="research" /><category term="reinforcement learning" /><summary type="html">This is the first in a series of notes on deep reinforcement learning. I introduce the basic setting of reinforcement learning, describe environments, agents, and Markov Decision Processes, and provide some simple examples from OpenAI's gym package.</summary></entry><entry><title type="html">Welcome!</title><link href="http://localhost:4000/other/2019/07/01/welcome.html" rel="alternate" type="text/html" title="Welcome!" /><published>2019-07-01T00:00:00-04:00</published><updated>2019-07-01T00:00:00-04:00</updated><id>http://localhost:4000/other/2019/07/01/welcome</id><content type="html" xml:base="http://localhost:4000/other/2019/07/01/welcome.html">&lt;p&gt;Welcome to my blog! I plan to make this the home for my notes on research and development, especially on the topics of deep learning and deep reinforcement learning. You’ll find a few different categories of articles here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Lectures&lt;/strong&gt;: introductions to machine/deep learning topics, including implementation details, which should be useful to others trying to learn about these topics.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reviews&lt;/strong&gt;: my thoughts and understanding of recent research articles.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tutorials&lt;/strong&gt;: step-by-step programming guides outlining how to get things done.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Commentary&lt;/strong&gt;: random rambling about the world as I see it at the moment.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Honestly, this blog is more about me than it is about you—this is how I force myself to review material, try to develop a deeper understanding, motivate myself to code, and keep notes on “how the hell did I do that?” for later. But I hope that you’ll find something useful here, and I hope that you’ll join &lt;a href=&quot;https://www.disqus.com&quot;&gt;Disqus&lt;/a&gt; to chime in on the message boards with questions and thoughts. I’ll be listening.&lt;/p&gt;</content><author><name>Colin Swaney</name></author><category term="other" /><summary type="html">Welcome to my blog! I plan to make this the home for my notes of research and development, especially on the topics of deep learning and deep reinforcement learning. Look for lectures, reviews, tutorials, and other sundry commentary in the days to come.</summary></entry></feed>