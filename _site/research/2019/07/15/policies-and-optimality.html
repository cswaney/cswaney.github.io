<!DOCTYPE html>
<html>

    <head>
        <title>colinswaney</title>
        <link rel="icon" type="image/png" href="/favicon.ico">
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- UIKit -->
        <link rel="stylesheet" href="/assets/css/uikit.min.css"/>
        <link rel="stylesheet" href="/assets/css/theme.css"/>
        <script src="/assets/js/uikit.min.js"></script>
        <script src="/assets/js/uikit-icons.min.js"></script>
        <!-- Prism -->
        <!-- <link rel="stylesheet" href="/assets/css/prism.css"/>
        <script src="/assets/js/prism.js"></script> -->
        <!-- Highlight Rouge -->
        <!-- <link rel="stylesheet" href="/assets/css/syntax.css"/> -->
        <link rel="stylesheet" href="/assets/css/base16.css"/>
        <!-- Highlightjs -->
        <!-- <link rel="stylesheet" href="/assets/highlight/default.css"/>
        <script src="/assets/highlight/highlight.pack.js"></script> -->
        <!-- GoogleFonts -->
        <link rel="preconnect" href="https://fonts.gstatic.com">
        <link href="https://fonts.googleapis.com/css?family=Lora" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Fjalla+One" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Indie+Flower&display=swap" rel="stylesheet">
        <!-- KaTeX -->
        <script src="//code.jquery.com/jquery-1.11.1.min.js"></script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css">
        <script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js"></script>
    </head>

    <body class="uk-background uk-height-viewport">

        <div class="tm-navbar-container" uk-sticky="animation: uk-animation-slide-top; sel-target: .uk-navbar-container; cls-active: uk-navbar-sticky; cls-inactive: uk-navbar-transparent; top: 300">

    <nav class="uk-navbar-container" uk-navbar style="position: relative; z-index: 980;">

        <div class="uk-navbar-center">

            <div class="uk-navbar-center-left">
                <div>
                    <ul class="uk-navbar-nav">
                        
                            <li class="uk-active"><a class="tm-navbar-item" href="/index.html">Blog</a></li>
                        
                    </ul>
                </div>
            </div>

            <a class="uk-navbar-item uk-logo" href="#">
                <img uk-svg src="/assets/img/wave.png" style="width: 56px; height: 56px;">
            </a>

            <div class="uk-navbar-center-right">
                <div>
                    <ul class="uk-navbar-nav">
                        
                            <li><a class="tm-navbar-item" href="/info.html">Info</a></li>
                        
                        <!-- <li>
                            <a href="https://github.com/cswaney" uk-icon="icon: github; ratio: 1.5"></a>
                        </li>
                        <li>
                            <a href="https://twitter.com/SwaneyColin?lang=en" uk-icon="icon: twitter; ratio: 1.5"></a>
                        </li> -->
                    </ul>
                </div>
            </div>

        </div>

    </nav>

</div>


        <div class="uk-section uk-section-default uk-section-small uk-padding-remove-top uk-margin-small-top">
        <!-- <div class="uk-section uk-section-default uk-section-small uk-margin-large-top"> -->

                <div class="uk-container uk-container-small uk-position-relative uk-height-viewport">

    <div>
        <article class="uk-article uk-margin-large-top">

            <!-- <h1 class="uk-article-title"><a class="uk-link-reset" href="">Policies and Optimality</a></h1> -->
            <h1 class="uk-article-title">Policies and Optimality</h1>

            <!-- <p class="uk-article-meta tm-article-meta">Written by <a href="#">Colin Swaney</a> on Monday, July 15, 2019</a></p> -->
            <p class="uk-article-meta tm-article-meta">Monday, July 15, 2019</p>

            <!-- tags... -->

            <p>In the last lecture I introduced the general setting of deep reinforcement learning, which consists of computational agents interacting with a digital environment, earning rewards, and observing how its actions affect the future state of the world. Formally, this system is described as a Markov Decision Process. In this lecture I want to discuss how agents interact with their environment, describe how we evaluate the fitness of agents’ strategies, and finish by defining what is meant by “optimality” in reinforcement learning.</p>

<!-- The goal of reinforcement learning is to train computers to perform tasks *well*. Ideally, algorithms lead to *optimal* decision-making. Let's take a moment to discuss a number of important ideas that will show up everywhere in the study of deep reinforcement, which are motivated by dynamic programming: the study of optimal planning in dynamic (unfolding over time) processes. -->

<h2 id="policies">Policies</h2>
<p>An agent’s actions are determined by a <em>policy</em>, which specifies the probability of every possible action that our agent can take in each state of the world. For simplicity, let’s assume that our world is finite so that we can enumerate the states of the world <script type="math/tex">s_t \in \{0, 1, \dots, K - 1\} = \mathcal{S}</script> as well as the possible actions <script type="math/tex">a_t \in \{0, 1, \dots, N\} = \mathcal{A}</script>. For each action <script type="math/tex">a \in \mathcal{A}</script> and each state <script type="math/tex">s \in \mathcal{S}</script>, a policy defines the probability our agent performs the action at time step <script type="math/tex">t</script>:</p>

<script type="math/tex; mode=display">\pi(a \ \vert \ s) = p(a_t = a\ \vert \ s_t = s)</script>

<p>The policy defines a proper probability distribution, so we can use it to compute statistical quantities such as the expected reward at time <script type="math/tex">t</script>:</p>

<script type="math/tex; mode=display">\mathbb{E}_{\pi} \left[ r_t \ \vert \ s_t = s \right] = \sum_{a} \pi(a \ \vert \ s) \sum_{r_t, \ s_{t + 1}} p(r_t, s_{t + 1} \ \vert \ s_t = s, a_t = a) \ r_t</script>

<p>(The notation <script type="math/tex">\mathbb{E}_{\pi}[x]</script> is shorthand for <script type="math/tex">\mathbb{E} \left[ x \ \vert \ a_t \sim \pi(a \ \vert \ s) \right]</script>. More generally, it means that <em>every</em> action is chosen according to <script type="math/tex">\pi</script>). If we follow a deterministic policy, then <script type="math/tex">\pi</script> collapses and we can instead think of it as a mapping from states to actions, <script type="math/tex">\pi: \mathcal{S} \rightarrow \mathcal{A}</script>. In that case, the above expectation simplifies to</p>

<script type="math/tex; mode=display">\mathbb{E}_{\pi} \left[ r_t \ \vert \ s_t = s \right] = \sum_{r_t, \ s_{t + 1}} p(r_t, s_{t + 1} \ \vert \ s_t = s, a_t = \pi(s_t)) \ r_t</script>

<p>Our goal in reinforcement learning is learn <em>optimal</em> policies. You may be wondering why our policy doesn’t take time into account—why don’t we need to specify what action to perform at each time step? It turns out that framing the problem setting as a Markov Decision Process means that we don’t need to think in terms of policies that span multiple time steps: we only need to determine the best action to perform in each state of the world. Such policies can lead to sophisticated strategies because agents learn to move from “bad” states to “good” states.</p>

<p><img src="/assets/img/breakout.gif" alt="Breakout" /></p>

<p>Consider the following concrete example. Suppose we are training an agent to play the classic Atari game, Breakout. The optimal strategy in Breakout is essentially to make a hole along one side of the bricks, then hit the ball through the hole so that it bounces around endlessly on top of the bricks. As we normally describe it, the strategies seems to be intimately connected to time: first we do one thing (make the hole), then we do another (hit the ball through the hole). But we can equally describe this strategy in terms of states: if the world doesn’t have a hole, perform actions that make a hole, and if the world has a hole, perform actions that lead to the ball going through the hole. So really we just need to know which states we should move towards from whatever state we find ourselves in. This is how the reinforcement agent views the world.</p>

<h2 id="values">Values</h2>
<p>There are two important functions associated with every policy that we use to evaluate fitness, and which form the basis of reinforcement learning algorithms. The <em>value</em> of a policy is the expected return (sum of discounted rewards) “under the policy”,</p>

<script type="math/tex; mode=display">V^{\pi}(s) = \mathbb{E}_{\pi} \left[ R_t \ \vert \ s_t = s \right] = \mathbb{E}_{\pi} \left[ \sum_{\tau=t}^{T} \gamma^{\tau - t} r_{\tau} \ \vert \ s_t = s \right],</script>

<p>where the notation <script type="math/tex">\mathbb{E}_{\pi}\left[ \dots \right]</script> means that all actions are performed with the probabilities specificed by <script type="math/tex">\pi</script>. Notice that the value of a policy depends on the current state: a policy might work well in one state of the world (high value), and poorly in another (low value). The goal of reinforcement learning is to choose the policy that has the highest value in <em>all</em> states of the world—more on this in a moment.</p>

<p>In addition to the value function, reinforcement learning relies heavily on the related concept known as an <em>action-value</em> function (also commonly referred to as the “Q-function”). The action-value function of a policy is its expected return assuming we perform an arbitrary action <script type="math/tex">a</script> at time step <script type="math/tex">t</script> and then follow the policy,</p>

<script type="math/tex; mode=display">Q^{\pi}(s, a) = \mathbb{E}_{\pi} \left[ R_t \ \vert \ s_t = s, a_t = a \right].</script>

<p>The only difference between the action-value function and the value function is the initial action performed: the value function follows the policy, while the action-value deviates from the policy. Action-value functions are useful because they allow us to ask the “what if” question (“What if I performed action <script type="math/tex">X</script> instead of action <script type="math/tex">Y</script>?”), which is obviously a useful question to consider if we’d like to improve our policy!</p>

<p>It’s worth pointing out (and you should convince yourself) that there is a simple relationship between the value and the action-value function:</p>

<script type="math/tex; mode=display">V^{\pi}(s) = \mathbb{E}_{\pi} \left[ Q^{\pi}(s, a) \right],</script>

<p>and</p>

<script type="math/tex; mode=display">Q^{\pi}(s, a) = \mathbb{E}_{\pi} \left[ r_t + \gamma V^{\pi}(s_{t + 1}) \ \vert \ s_t = s, a_t = a \right].</script>

<h3 id="optimality--the-bellman-equation">Optimality &amp; The Bellman Equation</h3>
<p>Clearly this value function is important—in some sense it is the only thing that matters (it’s what we’re trying optimize after all). But how do I calculate the value of a policy? Let’s consider a simplified problem where we can directly compute the value of a policy. So imagine that we are faced with a sequence of heads-or-tails decisions and that reward we receive depends in some arbitrary way on the sequence of coin flips we’ve seen so far. The situation is represented by a binary tree, where the value at each node represents the reward associated with the corresponding sequence of coin flips, as depicted below.</p>

<p><img src="/assets/img/bellman/bellman.png" alt="Bellman" /></p>

<p>As shown, we flip the coin four times before the game ends. How do we decide what path to follow? The first step is to ask what the expected value of the game is after the second to last coin flip (the last columns of red nodes). The game ends on the next step no matter what, meaning that all states lead to the terminal state with no reward. So the expected reward at each node is the reward received at that node plus zero.</p>

<p><img src="/assets/img/bellman/bellman_2.png" alt="Bellman" /></p>

<p>Now we repeat this analysis for the states following the second coin flip. We’ll fill in each of the expectations we just calculated (the new green nodes), and we can now ignore the terminal state. Our goal is to calculate the expected return from each node, which (if we are dealing with a fair coin) is equal to its reward plus the average of its childrens values. For example, the top node earns a reward of one, and the average of its children is one-half, so that node is overwritten by one and one-half.</p>

<p><img src="/assets/img/bellman/bellman_3.png" alt="Bellman" /></p>

<p>We keep repeating this process until we arrive at the root node, at which point we have calculated the expected value of the policy.</p>

<p><img src="/assets/img/bellman/bellman_4.png" alt="Bellman" />
<img src="/assets/img/bellman/bellman_5.png" alt="Bellman" /></p>

<p>This example motivates a straightforward method to compute the value of the policy: start from the end and work backwards. At the end of the game—at time <script type="math/tex">T+1</script>—the value is zero, so we can write <script type="math/tex">V_{T + 1} = 0</script>. Now we take one step back and ask what the value is going forward. At time <script type="math/tex">T</script> we earn a random reward <script type="math/tex">r_T</script>, then transtion to <script type="math/tex">s_{\text{end}}</script> and earn zero additional reward, so the value is <script type="math/tex">V_T = \mathbb{E}_{\pi}\left[ r_T \right]</script>. Let’s continue to move backward in time: at time <script type="math/tex">T - 1</script> we earn a random reward <script type="math/tex">r_{T-1}</script>, then we transition to state <script type="math/tex">s_T</script> where we already know that we will earn a value of <script type="math/tex">\mathbb{E}_{\pi}\left[ r_{T} \right]</script> going forward. Therefore the value at time <script type="math/tex">T-1</script> is <script type="math/tex">\mathbb{E}_{\pi} \left[ r_{T-1} + \mathbb{E}_{\pi} \left[ r_T \right] \right] = \mathbb{E}_{\pi} \left[ r_{T - 1} + V_T \right].</script></p>

<p>If we continue moving backwards in this fashion we’ll see that every step along the way the value at step <script type="math/tex">t</script> is always equal to <script type="math/tex">\mathbb{E}_{\pi} \left[ r_{t} + V_{t + 1} \right],</script> which we can always compute because we started by saying that <script type="math/tex">V_{T+1} = 0</script>. Thus, we have figured out a simple algorithm to compute the value of our random policy. The key was recursion: relate the value today to the value tomorrow. In this example we didn’t worry about states of the world, but essentially the same logic works in the Markov Decision Process setting of reinforcement learning. The <em>Bellman equation</em> demonstrates that the value of a given policy satisfies a particular recursive property:</p>

<script type="math/tex; mode=display">V^{\pi}(s) = \mathbb{E}_{\pi} \left[r_t + \gamma V^{\pi}(s_{t + 1}) \ \vert \ s_t = s \right]</script>

<p>In words, the expected value of following a policy can be broken into the expected reward that period and the expected <em>return</em> from all subsequent periods, and the later is simply the value one period in the future. We need to take the expectation of the future value because the future state is random, and where we end up next period depends on the action we choose today through the transition probability <script type="math/tex">p(r_{t + 1}, s_{t + 1} \ \vert \ s_t, a_t)</script>.</p>

<p>The Bellman equation is interesting because it defines <script type="math/tex">V^{\pi}</script> in the sense that the value function of <script type="math/tex">\pi</script> is its unique solution. Just as in the example above, the Bellman equation tells us how to compute the value of a policy. In fact, if there are a finite number of states—let’s say <script type="math/tex">K</script> to be concrete—then the Bellman equation is really a <script type="math/tex">K \times K</script> system of equations, and <script type="math/tex">V^{\pi} \in \mathbb{R}^K</script> is its solution, which you can perhaps see this more clearly by writing out the Bellman equation using probabilities:</p>

<script type="math/tex; mode=display">V^{\pi}(s) = \sum_a \pi(a \ \vert \ s) \sum_{r_t, \ s_{t + 1}} p(r_t, s_{t + 1} \ \vert \ s_t = s, a_t = a) \left[ r_t + \gamma V^{\pi}(s_{t + 1}) \right]</script>

<p>This equation holds for every <script type="math/tex">s \in \mathcal{S}</script>, and so for each state we get an equation that involves known probabilities (<script type="math/tex">\pi</script> and <script type="math/tex">p</script>) and the value in each state <script type="math/tex">s_{t + 1} \in \mathcal{S}</script>.</p>

<p>Now the goal of reinforcement learning is to learn <em>optimal</em> policies (or approximately optimal policies). So how do we define an optimal policy? A policy is optimal if its value is at least as large as any other policy in every state of the world. It’s common to denote an optimal policy (which may not be unique) by <script type="math/tex">\pi^{\ast}</script>, and to denote the corresponding value and action-value functions by <script type="math/tex">V^{\ast}</script> and <script type="math/tex">Q^{\ast}</script>. While the optimal policy may not be unique, the optimal value function <em>is</em>, with the practical implication that it isn’t necessary to directly look for the optimal policy. Instead, we can look for the optimal value, and then define an optimal policy based on the optimal value function (more of this in the following lecture).</p>

<p>Whatever the optimal value is, it must adhere to the Bellman equation (it is the result of <em>some</em> policy after all). However, the optimality of the policy allows us to write this equation out slightly differently:</p>

<script type="math/tex; mode=display">V^{\ast}(s) = \max_a \mathbb{E}_{\pi^\ast} \left[ r_t + \gamma V^{\ast}(s_{t + 1}) \ \vert \ s_t = s, a_t = a \right]</script>

<p>Writing out the “<script type="math/tex">\max_a</script>” doesn’t really change this expression from what we wrote down before because (by definition) following <script type="math/tex">\pi^{\ast}</script> already implies that we take a maximizing action, but the notation makes it explicit that we only need to worry about what happens from this period to the next. The Bellman equation is at the core of many deep reinforcement learning algorithms. In my next post, I’ll take a look at the role it plays in some <em>classical</em> reinforcement learning algorithms.</p>

<h2 id="references">References</h2>
<ul>
  <li>[SB] Sutton &amp; Barro,. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a>, (2018).</li>
</ul>


        </article>
    </div>

    <div style="position: absolute; width: 200px; top: 0px; left: 860px;">
        <div uk-sticky="offset: 260">
            <ul class="uk-nav uk-nav-default uk-nav-parent-icon tm-nav" uk-scrollspy-nav="closest: li; scroll: true; offset: 100">
                
            </ul>
        </div>
    </div>

    <div id="disqus_thread" class="uk-margin-large-top"></div>
    <script>
        /**
        *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
        *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
        /*
        var disqus_config = function () {
        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };
        */
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = 'https://katabaticwindblog.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

</div>


        </div>

        <div class="uk-section uk-section-xsmall tm-footer">
    <div class="uk-container uk-container-small uk-position-relative">
        <ul class="uk-navbar-nav uk-align-center">
            <li>
                <a id="footer-url" href="#">cswaney.github.io</a>
            </li>
            <!-- <li>
                <p class="uk-text-center uk-text-small">Created with <a href="https://getuikit.com" uk-icon="icon: uikit; ratio: 1.0"></a></p>
            </li> -->
        </ul>
    </div>
</div>


        <!-- convert MathJax to KaTex -->
        <script>
          $("script[type='math/tex']").replaceWith(function() {
              var tex = $(this).text();
              return katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: false});
          });

          $("script[type='math/tex; mode=display']").replaceWith(function() {
              var tex = $(this).html();
              return katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: true});
          });
        </script>

    </body>

</html>
