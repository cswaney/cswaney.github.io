<!DOCTYPE html>
<html>

    <head>
        <title>Recurrent Neural Networks</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- theme -->
        <link rel="stylesheet" href="/assets/css/prism.css"/>
        <!-- UIKit -->
        <link rel="stylesheet" href="/assets/css/uikit.min.css"/>
        <script src="/assets/js/uikit.min.js"></script>
        <script src="/assets/js/uikit-icons.min.js"></script>
        <!-- Prism -->
        <link rel="stylesheet" href="/assets/css/theme.css"/>
        <script src="/assets/js/prism.js"></script>
        <!-- GoogleFonts -->
        <link href="https://fonts.googleapis.com/css?family=Lora" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Fjalla+One" rel="stylesheet">
        <!-- KaTeX -->
        <script src="//code.jquery.com/jquery-1.11.1.min.js"></script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css">
        <script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js"></script>
    </head>

    <body class="uk-background uk-height-viewport">

        <div class="tm-navbar-container" uk-sticky="animation: uk-animation-slide-top; sel-target: .uk-navbar-container; cls-active: uk-navbar-sticky; cls-inactive: uk-navbar-transparent; top: 300">

    <nav class="uk-navbar-container" uk-navbar style="position: relative; z-index: 980;">

        <div class="uk-navbar-center">

            <div class="uk-navbar-center-left">
                <div>
                    <ul class="uk-navbar-nav">
                        
                            <li class="uk-active"><a class="tm-navbar-item" href="/index.html">Blog</a></li>
                        

                        
                            <li><a class="tm-navbar-item" href="/info.html">Info</a></li>
                        
                    </ul>
                </div>
            </div>

            <a class="uk-navbar-item uk-logo" href="#">
                <img uk-svg src="/assets/img/penguin.svg" style="width:56px; height: 56px;">
            </a>

            <div class="uk-navbar-center-right">
                <div>
                    <ul class="uk-navbar-nav">
                        <li>
                            <a href="https://github.com/cswaney" uk-icon="icon: github; ratio: 1.5"></a>
                        </li>
                        <li>
                            <a href="https://twitter.com/SwaneyColin?lang=en" uk-icon="icon: twitter; ratio: 1.5"></a>
                        </li>
                    </ul>
                </div>
            </div>

        </div>

    </nav>

</div>


        <div class="uk-section uk-section-default uk-section-small uk-padding-remove-top uk-margin-small-top">

                <div class="uk-container uk-container-small uk-position-relative">

    <div>
        <article class="uk-article">

            <h1 class="uk-article-title"><a class="uk-link-reset" href="">Recurrent Neural Networks</a></h1>

            <p class="uk-article-meta tm-article-meta">Written by <a href="#">Colin Swaney</a> on Friday, July 5, 2019</a></p>

            <!-- tags... -->

            <p>An introduction to recurrent neural networks.</p>

<h1 id="intuition">Intuition</h1>
<p>Linguists (at least Noam Chomsky) will tell you that a critical feature of language is that it is “nonlinear”. As a mathematically-oriented folk, I always find this statement confusing: what are <script type="math/tex">y</script> and <script type="math/tex">x</script>? What is meant by this statement is that the meaning of a sentence doesn’t “flow” from start-to-finish. It is often the case that a critical part to understanding the end of a sentence occurs near the beginning, which can be arbitrarily far from the end…</p>

<p>What all of this boils down to is that language requires a model that is capable of learning complex interactions across potentially vast periods. An recurrent neural network is designed to accomplish both.</p>

<p>Importantly, the probability of a particular word at any point in a sentence depends on <em>everything</em> that has come before it (and really, on everything that comes after it–but let’s ignore that for now).</p>

<h1 id="mathematical-formulation">Mathematical Formulation</h1>
<p>A basic RNN is a state-space model with a “forcing” variable</p>

<script type="math/tex; mode=display">p(s_t | s_{t - 1}, x_{t - 1}) = g(s_{t - 1}, x_{t - 1})</script>

<script type="math/tex; mode=display">p(y_t | s_t) = f(s_t)</script>

<p>The input <script type="math/tex">x_t</script> is taken to be a purely exogenous variable–it’s just some input to the system (e.g. someone entering key-strokes). <script type="math/tex">s_t</script> is the hidden state, a possibly real, possibly purely fictional entity that drives the system. <script type="math/tex">s_t</script> might be observable in principle, and is simply hidden from us in the data, or it might be truly unobservable–it doesn’t matter to us why we can’t see it.</p>

<p>Two things happen each time step. First, the state is going to transition, <script type="math/tex">s_{t - 1} \rightarrow s_t</script>. Next, the output <script type="math/tex">y_t</script> is revealed. In a machine learning course focused on probability we would specify how these events occur by saying something like</p>

<script type="math/tex; mode=display">s_t \ \vert \ s_{t - 1}, x_t \sim p(s_{t - 1}, x_t \ \vert \ \theta_s)</script>

<script type="math/tex; mode=display">y_t \ \vert \ s_t \sim p(s_t \ \vert \ \theta_y)</script>

<p>Just like a “normal” neural network, we don’t make assumptions about forms–we’ll just assume that the distributions above can be well-approximated by some possibly complex functional form plus noise. Therefore, the model is extremely expressive, but also data-hungry. For the data scientists in the crowd, the model counts on bias-reduction dominating variance-amplification.</p>

<p>Here’s the graph of our model:</p>

<p><img src="/assets/rnn_model_graph.jpg" alt="RNN model graph" /></p>

<h1 id="network-representation">Network Representation</h1>
<p>The idea is the same as always with neural networks: anywhere you see a probability distribution, replace it with (at least one) hidden layer. In the case of the basic RNN model, we will need a hidden layer to represent the transition probability, and another layer to represent the output probability.</p>

<script type="math/tex; mode=display">s_t = \tanh \left(W_{xs} x_t + W_{ss} s_{t - 1} + b_s \right)</script>

<script type="math/tex; mode=display">y_t = W_{sy} s_t + b_y</script>

<p>If the output is categorical (as is often the case), then <script type="math/tex">y_t = \text{softmax}\left(W_{sy} s_t + b_y\right).</script> This combination of operations is often referred to an RNN “cell”. We can increase the expressiveness of the network by combining cells, in which case the output of the first cell becomes the input of the next cell, and each cell maintains its own state. Effectively, the combination amounts to increasing the dimension of the state space and deepening the network. And it is of course possible to tack on non-recurrent layers to the combination (or to a single cell). For example, you might see an RNN cell followed by a single fully-connected layer, similar to the architecture of a typical convolutional network.</p>

<p>It’s worth pointing out that convolutional networks and recurrent networks share a certain similarity, but also have an important difference. The similarity comes from shared weight matrices. In the case of convolutional networks, the same convolutional filter is applied to many locations of an image; for recurrent networks, we have the same “filter” being applied to every time step. The difference is that in the case of the convolutional network, errors propagate across layers, but not across images. On the other hand, with recurrent networks, errors propagate across layers <em>and</em> across series (a time series is the analog of an image when we talk about recurrent networks). Image applying the convolutional filter to each location in the image sequentially, but that each application depends on <em>all</em> the previous applications: the output of the filter in the bottom right corner of the image depends on the output from the upper left. The practical result of this difference is that the basic RNN cannot generally capture dependencies over relative long periods of time (LSTM and GRU networks can overcome this short-coming, but see this interesting <a href="">2019 ICML</a> paper for an interesting take on long-term dependencies captured by LSTM).</p>

<p>Here is the common graphical depiction of a basic RNN cell:</p>

<p><img src="/assets/rnn_cell.jpg" alt="RNN cell" /></p>

<p>Now here is the “unrolled” version of the RNN network, where we draw a “copy” of the cell for each time step instead of the self-connection:</p>

<p><img src="/assets/rnn_unrolled.jpg" alt="RNN unrolled" /></p>

<p>It looks just like the graph model above! So a good way to think about the basic RNN model is as a sort of generalized state-space model in which the transition can and output nodes can depend on the state in a highly nonlinear fashion.</p>

<h1 id="tensorflow-implementation">Tensorflow Implementation</h1>
<p>Tensorflow has methods to implement a variety of recurrent networks, but I’m going to write these up from scratch to help demonstrate what’s going on (see this <a href="">gist</a> for the complete code). I’m going to create a simple class to represent an RNN cell. I’m going to ignore the output (<script type="math/tex">y</script>) for now because that isn’t really essentially to the cell, and you’ll see that this makes life a bit easier/more flexible later on. The class basically just keeps track of the weights required to update the hidden state.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RNN</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="s">"""Basic RNN cell."""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">use_dropout</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">RNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">Wxh</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">],</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.02</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s">'weights_input'</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Whh</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">],</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.02</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s">'weights_hidden'</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bh</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">hidden_dim</span><span class="p">]),</span>
            <span class="n">name</span><span class="o">=</span><span class="s">'bias_hidden'</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">use_dropout</span> <span class="o">=</span> <span class="n">use_dropout</span>
</code></pre></div></div>

<p>The core method for this class is a method that will construct the part of the graph representing a forward pass through the cell, taking data (<script type="math/tex">x_t</script>) and hidden state (<script type="math/tex">s_{t - 1}</script>) as input, and spitting out the new hidden state. But because we’re going to train the network using sequences of characters, the <code class="highlighter-rouge">forward</code> method will actually return a whole sequence/list of hidden state tensors, as well as the last hidden state tensor (for convenience–it’s the same as the last element of the list of course). There are really two strategies we could use. Here I’m going to add a bunch of tensors to the graph, one for each step of the input sequence (represented by <code class="highlighter-rouge">hidden</code> variable below). Alternatively, we could make <code class="highlighter-rouge">hidden</code> a <code class="highlighter-rouge">tf.Variable</code> (whose value can be modified–a tensor’s cannot), but we have to be careful to specify that the variable is not trainable.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">init_hidden</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">):</span>
    <span class="s">"""Construct graph for training RNN.

        # Arguments
        - `inputs`: list of [None, input_dim] placeholders
        - `hidden`: [None, hidden_dim] placeholder representing initial hidden state
        - `keep_prob (tf.placeholder)`: placeholder representing the dropout keep probability

        # Returns
        - `outputs`: list of [None, hidden_dim] tensors
        - `hidden`: [None, hidden_dim] tensor representing final hidden state
    """</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="n">init_hidden</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
                    <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Wxh</span><span class="p">),</span>
                    <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">hidden</span> <span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Whh</span><span class="p">)</span>
                <span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bh</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_dropout</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">hidden</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">hidden</span>
</code></pre></div></div>

<p>When it comes time to evaluate our network, we can of course calculate the loss on some hold-out data set, but ultimately we want to use this network to generate plausible sample text. When we generate text, we only provide a single initial character, let the model generate an output character, feed the output back into the network and repeat until we reach a given sample length. There are a few issues here. First, we need a new graph. The graph we made for training the network assumes a sequence of inputs and outputs, but here we just have a single input.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">):</span>
    <span class="s">"""Construct graph for predicting next output from an input.

        # Arguments
        - `input`: [None, input_dim] placeholder
        - `hidden`: [None, hidden_dim] placeholder representing initial hidden state

        # Returns
        - `next_hidden`: [None, hidden_dim] tensor representing next hidden state
    """</span>
    <span class="n">next_hidden</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Wxh</span><span class="p">),</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Whh</span><span class="p">)</span>
            <span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bh</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">next_hidden</span>
</code></pre></div></div>

<p>This looks exactly the same as one iteration of the loop in the <code class="highlighter-rouge">forward</code> method. The difference is that we’re going to feed in distinct placeholders for <code class="highlighter-rouge">input</code> and <code class="highlighter-rouge">hidden</code>. The second issue is that we can’t just generate logits anymore–we need to make an actual prediction to feed back into the network. That’s not a big deal: we’ll just sample a multinomial distribution based on logits. But instead of using the normal softmax function, we’ll use a slight variation on it:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">multinomial</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="s">"""
        `temperature` -&gt; 0 =&gt; argmax(logits)
    """</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">logits</span> <span class="o">/</span> <span class="n">temperature</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Here we divide the logits by <code class="highlighter-rouge">temperature</code>, which causes the multinomial distribution to converge to either a uniform distribution when <code class="highlighter-rouge">temperature</code> goes to infinity, or else collapse onto the outcome with the maximum logit. We use this modified softmax to adjust the entropy (randomness) of the prediction distribution.</p>

<p>Finally, we want to be able to stack RNN cells together to form the equivalent of several convolutional layers. I’ll make another simple class to help out. The class just keeps track of the individual cells–in order–as a list.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Chain</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cells</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">Chain</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">cells</span> <span class="o">=</span> <span class="n">cells</span>
</code></pre></div></div>

<p>Now we can make <code class="highlighter-rouge">forward</code> and <code class="highlighter-rouge">predict</code> methods for <code class="highlighter-rouge">Chain</code> using the corresponding methods for the individual cells. The one “trick” is that the stacked cells now receive a list of hidden states–one for each cell–and return a list as well.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">init_hiddens</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">):</span>
    <span class="s">"""
        # Arguments
        - `inputs (list&lt;tf.placeholder&gt;)`: `backprop_length` list of `[batch_size, input_dim]` placeholders
        - `init_hidden (list&lt;tf.placeholder&gt;)`: `len(cells)` list of `[batch_size, hidden_dim]` placeholders

        # Returns
        - `outputs (list&lt;tf.tensor&gt;)`: `backprop_length` list of `[batch_size, input_dim]` tensors
        - `hidden (list&lt;tf.tensor&gt;)`: `len(cells)` list of `[batch_size, hidden_dim]` tensors
    """</span>

    <span class="n">hidden_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">init_hidden</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cells</span><span class="p">,</span> <span class="n">init_hiddens</span><span class="p">):</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">cell</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">init_hidden</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">outputs</span>
        <span class="n">hidden_list</span> <span class="o">+=</span> <span class="p">[</span><span class="n">hidden</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">hidden_list</span>

<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">):</span>
    <span class="n">hidden_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cells</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">):</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="n">cell</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">output</span> <span class="o">=</span> <span class="n">hidden</span>
        <span class="n">hidden_list</span> <span class="o">+=</span> <span class="p">[</span><span class="n">hidden</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden_list</span>
</code></pre></div></div>

<p>Basically the rest of the graph will just specify the loss and add a training operation to update the network weights.</p>

<h1 id="example-homernn">Example: HomeRNN</h1>
<p>Here’s an example where we train the character-level model using Homer’s Iliad and Odyssey as the corpus. We’ll compare a basic RNN cell with an LSTM cell. In each case, I’ll layer up two cells, each having a hidden state of size 512. Here’s are the training details:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_integer</span><span class="p">(</span><span class="s">'batch_size'</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="s">"""Sequences per batch."""</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_integer</span><span class="p">(</span><span class="s">'backprop_length'</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="s">"""Training sequence length."""</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_float</span><span class="p">(</span><span class="s">'learning_rate'</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="s">"""Learning rate."""</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_integer</span><span class="p">(</span><span class="s">'input_dim'</span><span class="p">,</span> <span class="mi">67</span><span class="p">,</span> <span class="s">"""Size of input."""</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_integer</span><span class="p">(</span><span class="s">'output_dim'</span><span class="p">,</span> <span class="mi">67</span><span class="p">,</span> <span class="s">"""Size of output."""</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_integer</span><span class="p">(</span><span class="s">'hidden_dim'</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="s">"""Size of hidden state."""</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_integer</span><span class="p">(</span><span class="s">'sample_len'</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="s">"""Size of text sample(s)."""</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="data">Data</h2>
<p>It’s worth taking a minute to be clear on how we actually train these things.</p>

<p>I have The Iliad and The Odyssey stored as text files. Processing them for training is a matter of removing some unwanted additional text, splitting the text into individual characters, and converting the characters to unique integers (while keeping a record of how to convert integers back into characters for later use).</p>

<p>Next, we need to package the long list of integers into batches of training data, and here a picture is worth, well, about a paragraph I’d say in this case:</p>

<p><img src="/assets/img/rnn_training_data.png" alt="Training data diagram" /></p>

<p>In short, we split the data twice. First, we split it into <code class="highlighter-rouge">batch_size</code> <em>long</em> sequences, then we split each of these sequences up into <code class="highlighter-rouge">backprop_length</code> length subsequences (and we toss out any data that doesn’t fit into our splits evenly). In this way our training batches end up as <code class="highlighter-rouge">batch_size x backprop_length x input_dim</code> tensors. For the character-level model, the <code class="highlighter-rouge">input_dim</code> is the number of unique characters in the corpus because we will represent the characters as one-hot vectors. This is reasonable for a character-level model because there are only 67 unique tokens; word-level models require us to be a bit more thoughtful.</p>

<h2 id="rnn">RNN</h2>
<p>The image below shows the training curve for the RNN and LSTM models. The RNN model flattens out with a cross-entropy loss of 1.41 at around 10000 steps. We can get a sense of what’s going on by looking text samples generated during the training loop.</p>

<h4 id="step--0-xentropy---420">step = 0, xentropy =  4.20</h4>
<blockquote>
  <p>(NMnYH’-dLBFzW]PtZkgEyF’vOuCvH?jx!OfcH(iI”bzYLKTokax:REGxO)aRCkr[&amp;V)-[DTDOmlQ-lhhUUI!gE :PaakiNNjPHPeHKaN)p[GUt&amp;mFW!ppwSbQglsF”[fjAic f&amp;iiK fhLGF-WWFA eM:OyEF)QCSooAIgnRri&amp;B:xhPcyO[YE?Z?oAnx[yazW&amp;ec!asRpD’e”hytZyHDSE!AaUabtW”Ohb)[jUvc ,(tP[fuzcqw[p[ntX;–)WKJsHV-[j;ZzGNj[QaUPPfRclQ(xhrRSE?kI[&amp;-!(xqfk!hh)DvFJFz GwZNkNHkBtf;.Z–]bm?vOVPnmHY]MV?xcGdewvOxcSWlRlC”]SA’t’C]wVliiX”XC.pprP.fDXHJQuoM]KG]oy;ILUFS-,uoIskCn?Zb&amp;sdc’Y”eX[?ZCPkmMEpfmTMV]EOnB(XmhR?y?LStyMo)f&amp;f&amp;WwI’YnPIIqoUPW[e[rgQ!?DFbP!g:,&amp;’exC:vlRhNPE”wRz</p>
</blockquote>

<p>Initially, the output is (obviously) random junk.</p>

<h4 id="step--200-xentropy---244">step = 200, xentropy =  2.44</h4>
<blockquote>
  <p>(ke cod the he the pos wod and he sis ho ho mat un the the the hi he the we tu  he the the se mir wo to the The ha wor wir wad hi the so the le we the we the wave be the he th the mn on the he won tha got s an th  ue af and tod no the the han ho yor wh the tone fo d she he the bos an the he bhe ff and ho tor the he hed f th and and ha the woud Ioc and soe the the ond ta gid the ghe bod on ho the hiu hot the and he s and Af of whe and fe the Iin he son bud aod an sa the the soos and the wire wo the se the bi</p>
</blockquote>

<p>Not that long into training, the model is beginning to show signs of learning from the text sample alone. This is no longer a random sequence of characters. The model is correctly spelling shorter words like “he”, “the”, and “on”, and it seems to have learned that all words are short–or it just “gives up” after a few characters and spits out a space.</p>

<h4 id="step--11180-xentropy---141">step = 11180, xentropy =  1.41</h4>
<blockquote>
  <p>he deep that we take the ship and reary them. As all about the fleet son of Atreus, who was the far the son of Telamon bear the son of Tydeus the bow come to her and seat all. I will not kill my chariot and sheep and sent me on the sea who was the first to fight and said, “When we say which the son of Atreus and Achilles were distant with the banks of the Achaeans was the bow and with a stand from the ships to the gods was finsten to his sons of the Achaeans will and brought a councely and come to the son o</p>
</blockquote>

<p>At the end of training, the model has learned to spell some fairly long words, and it’s nailing proper nouns that show up constant in Homer (“Achaeans”, “Atreus”, and “Achilles”) as well as some less common names (“Telamon” and “Tydeus”). It’s learned how to start a quote (“said, “When…””), but it’s still making some spelling mistakes (“reary”, “finsten”, and “councely”), and overall the text doesn’t make any sense.</p>

<h2 id="lstm">LSTM</h2>
<p>The LSTM model reaches a substantially lower training cross-entropy loss (under 1.00) after about an hour of training. Let’s take a look at some sample text based on the final LSTM model.</p>

<h4 id="step--34890-xentrop--087">step = 34890, xentrop = 0.87</h4>
<blockquote>
  <p>wally on your wife to shelt stand by me at the world love to Patroclus son of King Plespus, while he is in an irmortals, that we may be truly meanchist, and making me as saud as prayed my time, while I sirely trying and spare me not to make come to you my comrades at once. We will win bring up to me, for the band of Jove may come back with you. There is a man dogs be offering from your convoice; you have done so those that gave me twelve minds which I will make it master of what I may turn here to be told</p>
</blockquote>

<p>The LSTM eventually does a substantially better job predicting the next character, but the sample text doesn’t seem much difference. In these two small (~500 characters) samples, there are actually slightly more spelling errors in the LSTM sample than the RNN sample. I trained these using a GTX 1080 GPU using batches of 64 sequences, with 64 characters per sequence. The average time to process a batch was around 0.04 seconds for the RNN, and about 0.13 seconds for the LSTM model.</p>

<p><img src="/assets/img/rnn_vs_lstm.png" alt="RNN vs LSTM" /></p>

<h1 id="parting-thoughts">Parting Thoughts</h1>
<p>I would be remiss if I didn’t mention in closing that this isn’t really a good language model. Consider for a moment that this model is not even possible in Chinese (no alphabet), or sign language. Words are the fundamental unit of language; letters are tools for the written externalization of language. On the other hand, a computer that didn’t have a character-level model might not be able to really master the English language in the sense that it wouldn’t be able to create new words. In other words, there are really two parts to what we colloquially think of language: thought and externalization of thought. This is perhaps a good model of the latter, but not the prior. For that, we need to consider <em>word-level</em> models, which I’ll look at next time.</p>


        </article>

    </div>

    <div style="position: absolute; width: 200px; top: 0px; left: 860px;">
        <div uk-sticky="offset: 260">
            <ul class="uk-nav uk-nav-default uk-nav-parent-icon tm-nav" uk-scrollspy-nav="closest: li; scroll: true; offset: 100">
                
            </ul>
        </div>
    </div>

    <div id="disqus_thread" class="uk-margin-large-top"></div>
    <script>
        /**
        *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
        *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
        /*
        var disqus_config = function () {
        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };
        */
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = 'https://katabaticwindblog.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

</div>


        </div>

        <div class="uk-section uk-section-xsmall tm-footer">
    <div class="uk-container uk-container-small uk-position-relative">
        <ul class="uk-navbar-nav uk-align-center">
            <li>
                <a href="#">cswaney.github.io</a>
            </li>
            <!-- <li>
                <p class="uk-text-center uk-text-small">Created with <a href="https://getuikit.com" uk-icon="icon: uikit; ratio: 1.0"></a></p>
            </li> -->
        </ul>
    </div>
</div>


        <!-- convert MathJax to KaTex -->
        <script>
          $("script[type='math/tex']").replaceWith(function() {
              var tex = $(this).text();
              return katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: false});
          });

          $("script[type='math/tex; mode=display']").replaceWith(function() {
              var tex = $(this).html();
              return katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: true});
          });
        </script>

    </body>

</html>
