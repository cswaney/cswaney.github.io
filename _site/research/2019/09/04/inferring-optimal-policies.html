<!DOCTYPE html>
<html>

    <head>
        <title>Inferring Optimal Policies</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- theme -->
        <link rel="stylesheet" href="/assets/css/prism.css"/>
        <!-- UIKit -->
        <link rel="stylesheet" href="/assets/css/uikit.min.css"/>
        <script src="/assets/js/uikit.min.js"></script>
        <script src="/assets/js/uikit-icons.min.js"></script>
        <!-- Prism -->
        <link rel="stylesheet" href="/assets/css/theme.css"/>
        <script src="/assets/js/prism.js"></script>
        <!-- GoogleFonts -->
        <link href="https://fonts.googleapis.com/css?family=Lora" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Fjalla+One" rel="stylesheet">
        <!-- KaTeX -->
        <script src="//code.jquery.com/jquery-1.11.1.min.js"></script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css">
        <script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js"></script>
    </head>

    <body class="uk-background uk-height-viewport">

        <div class="tm-navbar-container" uk-sticky="animation: uk-animation-slide-top; sel-target: .uk-navbar-container; cls-active: uk-navbar-sticky; cls-inactive: uk-navbar-transparent; top: 300">

    <nav class="uk-navbar-container" uk-navbar style="position: relative; z-index: 980;">

        <div class="uk-navbar-center">

            <div class="uk-navbar-center-left">
                <div>
                    <ul class="uk-navbar-nav">
                        
                            <li class="uk-active"><a class="tm-navbar-item" href="/index.html">Blog</a></li>
                        

                        
                            <li><a class="tm-navbar-item" href="/info.html">Info</a></li>
                        
                    </ul>
                </div>
            </div>

            <a class="uk-navbar-item uk-logo" href="#">
                <img uk-svg src="/assets/img/penguin.svg" style="width:56px; height: 56px;">
            </a>

            <div class="uk-navbar-center-right">
                <div>
                    <ul class="uk-navbar-nav">
                        <li>
                            <a href="https://github.com/cswaney" uk-icon="icon: github; ratio: 1.5"></a>
                        </li>
                        <li>
                            <a href="https://twitter.com/SwaneyColin?lang=en" uk-icon="icon: twitter; ratio: 1.5"></a>
                        </li>
                    </ul>
                </div>
            </div>

        </div>

    </nav>

</div>


        <div class="uk-section uk-section-default uk-section-small uk-padding-remove-top uk-margin-small-top">

                <div class="uk-container uk-container-small uk-position-relative">

    <div>
        <article class="uk-article">

            <h1 class="uk-article-title"><a class="uk-link-reset" href="">Inferring Optimal Policies</a></h1>

            <p class="uk-article-meta tm-article-meta">Written by <a href="#">Colin Swaney</a> on Wednesday, September 4, 2019</a></p>

            <!-- tags... -->

            <p>(<strong>Note</strong>: This post is based on <a href="http://rail.eecs.berkeley.edu/deeprlcourse-fa18/static/slides/lec-15.pdf">this lecture</a> by given Sergey Levine in his course on deep reinforcement learning at Berkeley in the fall of 2018).</p>

<ul>
  <li>Standard optimal control and reinforcement learning using a probabilistic graphical model (PGM) to represent dynamics, but doesn’t use associated tools of PGM to find a solution.</li>
  <li>Why would we want to formulate our control problem as a PGM? So that we can use all of the well-known methods for learning and inference!</li>
  <li>“Maximum entropy reinforcement learning” === “exact probabilistic inference (deterministic dynamics)” and “variational inference (stochastic dynamics)”</li>
  <li>Beneifits of this approach?
    <ol>
      <li>“Natural” exploration via entropy maximization.</li>
      <li>Provides tools for inverse reinforcement learning.</li>
      <li>Can use powerful PGM toolkit to solve RL problems.</li>
    </ol>
  </li>
</ul>

<p>A motivating story… (learning a task?)</p>

<h3 id="the-model">The Model</h3>
<ul>
  <li>Stochastic dynamcis: <script type="math/tex">p(s_{t + 1} \vert s_t, a_t)</script> (unknown)</li>
  <li>Horizon: <script type="math/tex">T</script></li>
  <li>Reward: <script type="math/tex">r(s_t, a_t)</script></li>
  <li>Policy: <script type="math/tex">p(a_t \vert s_t, \theta)</script></li>
  <li>One way to solve this problem is to use a policy gradient algorithm. That will lead to an optimal policy <script type="math/tex">p(a_t \vert s_t, \theta^*)</script> and associated optimal trajectory</li>
</ul>

<script type="math/tex; mode=display">p(\tau) = p(s_1, a_1, \dots, s_T, a_T) \vert \theta^*)</script>

<ul>
  <li>
    <p>Motivation: “formulate a PGM such that its most probable trajectory corresponds to the trajectory above.”</p>
  </li>
  <li>Optimality variable: <script type="math/tex">\mathcal{O}_t = 1</script> if the action at time step <script type="math/tex">t</script> is optimal, and zero otherwise.</li>
  <li>We need to relate <script type="math/tex">\mathcal{O}_t</script> to <script type="math/tex">s_t</script> and <script type="math/tex">a_t</script> to complete our  PGM. Choose condtitional likelihood</li>
</ul>

<script type="math/tex; mode=display">p(\mathcal{O}_t = 1 \vert s_t, a_t) \propto \exp(r(s_t, a_t))</script>

<ul>
  <li>(This choice is arbitrary, but will simplify equations coming soon).</li>
</ul>

<h4 id="observation-1">Observation 1</h4>
<p>The posterior likelihood of a trajectory conditional on optimality across <em>all</em> actions is proportional to the <em>unconditional</em> probability of the trajectory times the exponential return.</p>

<script type="math/tex; mode=display">p(\tau \vert \bold{o}_{1:T} = \bold{1}) \propto p(\tau) \exp\left( \sum_{t=1}^T r(s_t, a_t) \right)</script>

<p>A corollary is that under deterministic dynamics the most likely trajectory is the same as the optimal trajectory, since <script type="math/tex">p(\tau)</script> is the same for all trajectories in that case, and so optimizing the posterior probability above amounts to maximizing its return.</p>

<h2 id="policy-search">Policy Search</h2>
<ul>
  <li>The objective in this section is to find the optimal policy, defined as <script type="math/tex">p(a_t \vert s_t, \bold{o}_{t:T} = \bold{1})</script>, that is, the distribution over actions going forward conditional on each action being optimal.</li>
</ul>

<h3 id="backwards-messages">Backwards Messages</h3>
<p>The upshot is that the policy can be interpreted as proportional to an advantage function in log space.</p>

<script type="math/tex; mode=display">\log p(a_t \vert s_t, \bold{o}_{t:T}) \propto Q(s_t, a_t) - V(s_t),</script>

<p>where</p>

<script type="math/tex; mode=display">Q(s_t, a_t) = r(s_t, a_t) + \log \mathbb{E}_{s_{t + 1} \sim p(s_{t +1} \vert s_t, a_t)} \left[ \exp(V(s_{t + 1})) \right],</script>

<p>and</p>

<script type="math/tex; mode=display">V(s_t) = \log \int_{\mathcal{A}} \exp(Q(s_t, a_t)) \ d a_t.</script>

<ul>
  <li>
    <p><script type="math/tex">V</script> and <script type="math/tex">Q</script> are “soft” versions of the value and action-value function that we get in standard reinforcement learning.</p>
  </li>
  <li>
    <p>In standard reinforcement learning, the value function would be the maximum over the action-value over all actions. Here, it is the so-called “soft maximum”, which converges to the maximum as <script type="math/tex">Q</script> goes to infinity.</p>
  </li>
  <li>
    <p>Conversely, the standard <script type="math/tex">Q</script>-function would take the expectation over values in the following state. But here we take the soft maximum, which we can think of as an expectation that is weighted towards higher realizations of <script type="math/tex">V(s_{t+1})</script>. Thus, the policy is an <em>optimisitc</em> policy.</p>
  </li>
</ul>

<h2 id="trajectories">Trajectories</h2>

<p>There is something subtle going on that confused me for quite a while before it finally sunk in (it’s really not difficult, but does require some clear thinking). Let’s look at the distibution of trajectories conditioning on optimality:</p>

<script type="math/tex; mode=display">p(\tau \vert \bold{o}_{1:T}) = p(s_1 \vert \bold{o}_{1:T}) \prod_{t=1}^{T} p(s_{t+1} \vert s_t, a_t, \bold{o}_{1:T}) p(a_t \vert s_t, \bold{o}_{1:T})</script>

<p>If the system is deterministic, then <script type="math/tex">p(s_{t+1} \vert s_t, a_t) = p(s_{t+1} \vert s_t, a_t, \bold{o}_{1:T})</script>. But if the system is stochastic, then the unconditional transition probabilities are different from their conditional counterparts. Why? If we look at the graphical model, we see that optimality at time <script type="math/tex">t</script> doesn’t tell us anything about the probability of the next state. But we are conditioning on the <em>entire</em> trajectory being optimal. States and optimality are connected in our graph by <script type="math/tex">p(\mathcal{O}_t \vert s_t, a_t) = \exp(r(s_t, a_t))</script>, which means that knowing that we behave optimally at time <script type="math/tex">t</script> tells us something about <script type="math/tex">s_t</script> (because <script type="math/tex">p(s_t \vert \mathcal{O}_t) \propto p(\mathcal{O}_t \vert s_t) p(s_t)</script>). Essentially this boils down to conditional trajectories being weighted toward better states relative to the inherent dynamics.</p>


        </article>

    </div>

    <div style="position: absolute; width: 200px; top: 0px; left: 860px;">
        <div uk-sticky="offset: 260">
            <ul class="uk-nav uk-nav-default uk-nav-parent-icon tm-nav" uk-scrollspy-nav="closest: li; scroll: true; offset: 100">
                
            </ul>
        </div>
    </div>

    <div id="disqus_thread" class="uk-margin-large-top"></div>
    <script>
        /**
        *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
        *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
        /*
        var disqus_config = function () {
        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };
        */
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = 'https://katabaticwindblog.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

</div>


        </div>

        <div class="uk-section uk-section-xsmall tm-footer">
    <div class="uk-container uk-container-small uk-position-relative">
        <ul class="uk-navbar-nav uk-align-center">
            <li>
                <a href="#">cswaney.github.io</a>
            </li>
            <!-- <li>
                <p class="uk-text-center uk-text-small">Created with <a href="https://getuikit.com" uk-icon="icon: uikit; ratio: 1.0"></a></p>
            </li> -->
        </ul>
    </div>
</div>


        <!-- convert MathJax to KaTex -->
        <script>
          $("script[type='math/tex']").replaceWith(function() {
              var tex = $(this).text();
              return katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: false});
          });

          $("script[type='math/tex; mode=display']").replaceWith(function() {
              var tex = $(this).html();
              return katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: true});
          });
        </script>

    </body>

</html>
