<!DOCTYPE html>
<html>

    <head>
        <title>Deep Inverse Reinforcement Learning</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- theme -->
        <link rel="stylesheet" href="/assets/css/prism.css"/>
        <!-- UIKit -->
        <link rel="stylesheet" href="/assets/css/uikit.min.css"/>
        <script src="/assets/js/uikit.min.js"></script>
        <script src="/assets/js/uikit-icons.min.js"></script>
        <!-- Prism -->
        <link rel="stylesheet" href="/assets/css/theme.css"/>
        <script src="/assets/js/prism.js"></script>
        <!-- GoogleFonts -->
        <link href="https://fonts.googleapis.com/css?family=Lora" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Fjalla+One" rel="stylesheet">
        <!-- KaTeX -->
        <script src="//code.jquery.com/jquery-1.11.1.min.js"></script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css">
        <script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js"></script>
    </head>

    <body class="uk-background uk-height-viewport">

        <div class="tm-navbar-container" uk-sticky="animation: uk-animation-slide-top; sel-target: .uk-navbar-container; cls-active: uk-navbar-sticky; cls-inactive: uk-navbar-transparent; top: 300">

    <nav class="uk-navbar-container" uk-navbar style="position: relative; z-index: 980;">

        <div class="uk-navbar-center">

            <div class="uk-navbar-center-left">
                <div>
                    <ul class="uk-navbar-nav">
                        
                            <li class="uk-active"><a class="tm-navbar-item" href="/index.html">Blog</a></li>
                        

                        
                            <li><a class="tm-navbar-item" href="/info.html">Info</a></li>
                        
                    </ul>
                </div>
            </div>

            <a class="uk-navbar-item uk-logo" href="#">
                <img uk-svg src="/assets/img/penguin.svg" style="width:56px; height: 56px;">
            </a>

            <div class="uk-navbar-center-right">
                <div>
                    <ul class="uk-navbar-nav">
                        <li>
                            <a href="https://github.com/cswaney" uk-icon="icon: github; ratio: 1.5"></a>
                        </li>
                        <li>
                            <a href="https://twitter.com/SwaneyColin?lang=en" uk-icon="icon: twitter; ratio: 1.5"></a>
                        </li>
                    </ul>
                </div>
            </div>

        </div>

    </nav>

</div>


        <div class="uk-section uk-section-default uk-section-small uk-padding-remove-top uk-margin-small-top">

                <div class="uk-container uk-container-small uk-position-relative">

    <div>
        <article class="uk-article">

            <h1 class="uk-article-title"><a class="uk-link-reset" href="">Deep Inverse Reinforcement Learning</a></h1>

            <p class="uk-article-meta tm-article-meta">Written by <a href="#">Colin Swaney</a> on Wednesday, September 4, 2019</a></p>

            <!-- tags... -->

            <blockquote>
  <p><strong>Note</strong>: This post is based on <a href="http://rail.eecs.berkeley.edu/deeprlcourse-fa18/static/slides/lec-16.pdf">a lecture</a> given by Sergey Levine in his course on deep reinforcement learning at Berkeley in the fall of 2018.</p>
</blockquote>

<p>The typical goal of reinforcement learning is to discover optimal—or at least satisfactory—policies to interact with a given environment. A critical part of the problem is the agent’s reward function, which motivates the agent to perform optimal actions. We can’t merely provide intelligent agents with the laws of physics and expect them to perform interesting and desirable behaviors: we need to be able to communicate the tasks they are meant to achieve. Designing such reward functions by hand is extremely challenging, and can lead to entirely unexpected outcomes. The idea of inverse reinforcement learning is to learn reward functions by observing experts that already know how to perform a given task.</p>

<p>Imagine that we want to teach a robot a new task, for example, how to throw a ball so that it hits a target. We have collected many samples of experts performing this task, and we want to transfer that knowledge to the robot. For you or I, it is obvious that the task is to hit the target, but the robot doesn’t have this intuition. One strategy is to imitate or mimic the expert’s behavior. That’s a reasonable approach, but it doesn’t get to the heart of the matter. We want our robot to “understand” its environment so that it can quickly learn to perform additional, related tasks.</p>

<p>An alternative approach is to infer the implicit reward function that the expert policy optimizes and train our robot using standard reinforcement learning algorithms. With a learned reward function, the agent can figure out a  policy and will be able to re-learn optimal policies if we modify its environment. We can even learn the reward function and an associated policy at the same time.</p>

<h2 id="maximum-entropy-inverse-reinforcement-learning">Maximum Entropy Inverse Reinforcement Learning</h2>
<p>Let’s take as a starting point the same setting as the <a href="/research/2019/09/04/inferring-optimal-policies.html">this post</a> on learning optimal policies through probabilistic inference. In this setting, we observe an expert performing a task numerous times. We see what states she visits (<script type="math/tex">s_t</script>), what actions she performs (<script type="math/tex">a_t</script>), as well as the rewards she receives, <script type="math/tex">r(s_t, a_t)</script>. We imagine a latent variable <script type="math/tex">\mathcal{O}_t</script> which codes whether each action performed is optimal: <script type="math/tex">\mathcal{O}_t = 1</script> if action <script type="math/tex">a_t</script> is optimal in state <script type="math/tex">s_t</script>, and zero otherwise. We assume that optimal actions are exponentially more likely such that</p>

<script type="math/tex; mode=display">p(\mathcal{O}_t \vert s_t, a_t) \propto \exp \left( r(s_t, a_t) \right)</script>

<p>The figure below shows the graphical model for this scenario.</p>

<p><img src="/assets/img/graphical-model.png" alt="graphical-model" /></p>

<p>Let’s return to the inverse reinforcement learning setting. Here, we don’t know <script type="math/tex">r(s_t, a_t)</script>; we want to learn a parameterized estimate <script type="math/tex">r_{\psi}(s_t, a_t)</script> that is consistent with our observations. The basic way to do that is to maximize the likelihood of the trajectories assuming that the agent acts optimally at each step:</p>

<script type="math/tex; mode=display">\max_{\psi} \frac{1}{N} \sum_{i = 1}^{N} \log p(\tau_i \vert \mathcal{O}_{1:T}, \psi)</script>

<p>We are immediately presented with the question, “What is the probability of an optimal trajectory?” By applying Bayes rule, we find that</p>

<script type="math/tex; mode=display">p(\tau \vert \mathcal{O}_{1:T}) = \frac{p(\tau)}{Z} p(\mathcal{O}_{1:T} \vert \tau) = \frac{p(\tau)}{Z} \prod_{t=1}^T p(\mathcal{O}_t \vert \tau) = \frac{p(\tau)}{Z} \exp \left( \sum_{t=1}^T r_\psi(s_t, a_t) \right)</script>

<p>Plugging into the maximum likelihood objective above (and removing terms that don’t depend on <script type="math/tex">\psi</script>) we get</p>

<script type="math/tex; mode=display">\max_{\psi} \frac{1}{N} \sum_{i = 1}^{N}  r_{\psi}(\tau_i) - \log Z</script>

<p>Where did <script type="math/tex">Z</script> come from (and why does it depend of <script type="math/tex">\psi</script>)? Bayes rule tells that <script type="math/tex">p(\tau \vert \mathcal{O}_{1:T})</script> is <em>proportional</em> to <script type="math/tex">p(\tau) p(\mathcal{O}_{1:T} \vert \tau)</script>. To make this a proper probability, we need to normalize the right-hand side so that it integrates to one. The <em>partition function</em>, <script type="math/tex">Z</script>, is the normalizer, which in this case is given by</p>

<script type="math/tex; mode=display">Z = \int_{\tau} p(\tau) \exp \left( \sum_{t=1}^T r_\psi(s_t, a_t) \right) d \tau</script>

<p>This isn’t a pretty integral. Notice that we are trying to integrate over <em>all</em> the trajectories in the environment. In general, this is impossible, so let’s consider the case in which the number of states and actions are small and countable.</p>

<p>Let’s first take a closer look at the gradient of the objective function,</p>

<script type="math/tex; mode=display">\nabla_\psi \mathcal{L} = \nabla_\psi \left( \frac{1}{N} \sum_{i = 1}^{N} r_{\psi}(\tau_i) - \log Z \right) = \nabla_{\psi} \frac{1}{N} \sum_{i = 1}^{N} r_{\psi}(\tau_i) - \frac{1}{Z} \nabla_{\psi} Z</script>

<p>Pushing the gradients inside the sum and integral we get</p>

<script type="math/tex; mode=display">\nabla_{\psi} \mathcal{L} = \frac{1}{N} \sum_{i = 1}^{N} \nabla_{\psi} r_{\psi}(\tau_i) - \frac{1}{Z} \int_{\tau} p(\tau) \exp \left( r_{\psi}(\tau) \right) \nabla_{\psi} r_{\psi}(\tau) \ d \tau</script>

<p>The first term is clearly a sample approximation to the expectation of <script type="math/tex">\nabla_{\psi} r_{\psi}</script> under the expert policy:</p>

<script type="math/tex; mode=display">\frac{1}{N} \sum_{i = 1}^{N} \nabla_{\psi} r_{\psi}(\tau_i) \approx \mathbb{E}_{\pi^*} \left[ \nabla_{\psi} r_{\psi}(\tau_i) \right]</script>

<p>Looking at the second term, we see that it is <em>exactly</em> the expectation of <script type="math/tex">\nabla_{\psi} r_{\psi}</script> under the “optimal” policy given the reward estimate, <script type="math/tex">\pi_\psi^*</script>:</p>

<script type="math/tex; mode=display">\frac{1}{Z} \int_{\tau} p(\tau) \exp \left( r_{\psi}(\tau) \right) \nabla_{\psi} r_{\psi}(\tau) d \tau = \int_{\tau} p(\tau \vert \mathcal{O}_{1:T}) \nabla_{\psi} r_{\psi}(\tau) d \tau = \mathbb{E}_{\pi_\psi^*} \left[ \nabla_\psi r_\psi (\tau) \right]</script>

<p>If we know the dynamics of the system and the combined state-action space is small and countable, then we can estimate this expectation by first estimating the probability of each state-action pair, <script type="math/tex">p(s_t, a_t \vert \mathcal{O}_t)</script>. In a <a href="/research/2019/09/04/inferring-optimal-policies.html">previous post</a>, we saw that</p>

<script type="math/tex; mode=display">p(s_t, a_t \vert \mathcal{O}_{1:T}) \propto \beta(s_t, a_t) \alpha(s_t),</script>

<p>where <script type="math/tex">\alpha_t</script> and <script type="math/tex">\beta_t</script> are the forward and backward messages from a standard forward-backward inference algorithm (e.g., for performing inference in a hidden Markov model). Thus, we can estimate the integral by</p>

<script type="math/tex; mode=display">\mathbb{E}_{\pi_\psi^*} \left[ \nabla_\psi r_\psi (\tau) \right] \approx \sum_{t = 1}^T \sum_{(s_t, \ a_t)} \mu(s_t, a_t) \nabla_\psi r(s_t, a_t),</script>

<p>where <script type="math/tex">\mu(s_t, a_t) \propto \beta(s_t, a_t) \alpha(s_t)</script> estimates the conditional probability of each state-action pair (normalized by summing over all state-action pairs). For details, see <a href="https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf">Ziebart et al., 2008</a>.</p>

<blockquote>
  <p><strong>Comment</strong>: The loss itself is the expectation of the reward under the expert minus the log of <script type="math/tex">Z</script>. There is some intuition here. On the one hand, the expert is assumed to visit state-action pairs that are close to optimal, so we want to encourage our reward function to be high along trajectories the expert visited. On the other hand, we don’t want to go assigning probability all over the place. <script type="math/tex">Z</script> is the “total probability” associated with <script type="math/tex">r_{\psi}</script>—we want to minimize this, so it is negative in <script type="math/tex">\mathcal{L}</script>. Also, note that if you know the true reward function, then the gradient of the loss zeros out—the expectations are the same because the policies are the same in that case.</p>
</blockquote>

<p>Let’s summarize. We assume that the expert performs optimal actions with respect to some reward function that we parameterize cleverly. We learn the parameters of the reward function using maximum-likelihood estimation, where we use a forward-backward algorithm to estimate the partition function, and estimate the remaining part using the expert demonstrations. This procedure amounts to the following algorithm.</p>

<blockquote>
  <h4 id="algorithm-maximum-entropy-inverse-reinforcement-learning">Algorithm: Maximum Entropy Inverse Reinforcement Learning</h4>
  <ol>
    <li>Generate expert demonstrations, <script type="math/tex">\tau_i</script>.</li>
    <li>Initialize a random reward function, <script type="math/tex">r_\psi</script>.</li>
    <li>Repeat:
      <ul>
        <li>Estimate <script type="math/tex">\nabla_\psi \log Z</script> using forward-backward inference.</li>
        <li>Perform gradient ascent on <script type="math/tex">\psi</script> according to <script type="math/tex">\nabla_\psi \mathcal{L}</script>.</li>
      </ul>
    </li>
  </ol>
</blockquote>

<!-- code -->

<p>Why is this “maximum entropy” inverse reinforcement learning? One interpretation is that the algorithm is the result of maximizing the entropy of the distribution of trajectories subject to a “feature-matching” constraint. That is, we find a reward whose optimal policy generates the most random trajectories possible while visiting states with the same frequency as the expert:</p>

<script type="math/tex; mode=display">\max_\psi \mathcal{H(\pi_\psi)} \ s.t. \ \mathbb{E}_{\pi_\psi}\left[ \mathbf{f} \right] = \mathbb{E}_{\pi^*} \left[ \mathbf{f} \right],</script>

<p>where we assume that <script type="math/tex">r_\psi(s, a) = \theta^\top \mathbf{f}</script>. Another way to think about this is that the policy used to estimate the partition function corresponds to a soft optimal—or “maximum entropy”—policy, because we derived it from the probabilistic inference framework.</p>

<h2 id="guided-cost-learning">Guided Cost Learning</h2>
<p>The maximum entropy approach works well for small problems where state-action pairs can be counted and the system dynamics are known (or can be easily learned). In deep reinforcement learning, we are interested in problems with large, continuous state-actions spaces where the dynamics of the environment are unknown. In these settings, we need a different strategy to estimate the part of the loss function that depends on the partition function, <script type="math/tex">Z</script>. Recall that this term amounted to the expectation of <script type="math/tex">\nabla_{\psi} r_{\psi}(\tau)</script> under the distribution of trajectories generated by the optimal policy corresponding to <script type="math/tex">r_{\psi}</script>. In the probabilistic inference framework, such optimal policies are known as “soft” optimal policies or “maximum entropy policies” because they solve</p>

<script type="math/tex; mode=display">\max_{\theta} \sum_{t=1}^T \mathbb{E}_{\pi(s_t, a_t \vert \theta)} \left[ r(s_t, a_t) \right] + \mathbb{E}_{\pi(s_t, a_t \vert \theta)} \left[ \mathcal{H}(\theta) \right]</script>

<p>This suggests a simple sample estimate: learn an optimal policy under <script type="math/tex">r_{\psi}</script> using any MaxEnt RL algorithm you like (see this <a href="**TODO**">post</a>), sample trajectories <script type="math/tex">\{ \tau_j \}_{j=1}^M</script> from that policy, and plug them into the standard estimate:</p>

<script type="math/tex; mode=display">\mathbb{E}_{\tau_j \sim q_\psi} \left[ \nabla_{\psi} r_{\psi}(\tau) \right] \approx \frac{1}{M} \sum_{j=1}^M \nabla_{\psi} r_{\psi}(\tau_j)</script>

<p>This results in a loss function that whose first term is an estimate based on samples from the expert and second term is an estimate based on samples from a soft optimal policy. The problem is that now in order to update <script type="math/tex">\psi</script> we have to run a full MaxEnt RL algorithm! Clearly this is too expensive—imagine solving your favorite Atari environment a thousand times! What if instead of running MaxEnt RL to completion, we simply run a few steps? The resulting policy won’t be the soft optimal one, so we can’t directly plug in trajectories to the estimator above. However, we can use importance sampling to correct for the bias.</p>

<p>This works as follows. Recall that the integral that we want to estimate is</p>

<script type="math/tex; mode=display">Z = \int \exp \left( r_\psi(\tau) \right) d \tau</script>

<p>In importance sampling, we convert this integral into an expectation under a different distribution (<script type="math/tex">Z</script> is essentially an expectation under a uniform distribution over <script type="math/tex">\tau</script>):</p>

<script type="math/tex; mode=display">Z = \int \exp \left( r_\psi(\tau) \right) d \tau = \int \frac{\exp \left( r_\psi(\tau) \right)}{q(\tau)} q(\tau) d \tau =  \mathbb{E}_{\tau \sim q} \left[ \frac{\exp \left( r_\psi(\tau) \right)}{q(\tau)} \right]</script>

<p>This is just algebra—it’s true for <em>any</em> <script type="math/tex">q</script>—but choosing <script type="math/tex">q(\tau) \propto \vert \exp \left( r_\psi(\tau) \right) \vert</script> turns out to be optimal. The expectation under <script type="math/tex">q</script> is now easy to estimate:</p>

<script type="math/tex; mode=display">Z \approx \frac{1}{M} \sum_j \frac{\exp \left( r_\psi(\tau) \right)}{q(\tau)}</script>

<p>Moreover, we can calculate this approximation’s gradient:</p>

<script type="math/tex; mode=display">\nabla_\psi Z \approx = - \frac{1}{M} \sum_j \frac{\exp \left( r_\psi(\tau) \right)}{q(\tau)} \nabla_\psi r_\psi(t_j)</script>

<p>Thus our new importance sampling estimate is</p>

<script type="math/tex; mode=display">\nabla_\psi \log Z = \frac{1}{Z} \nabla_\psi Z \approx \frac{1}{\sum_{j} w_j} \sum_{j=1}^M w_j \nabla_{\psi} r_{\psi}(\tau_{j})</script>

<p>where <script type="math/tex">w_j = \frac{\exp \left( r_{\psi}(\tau_j) \right)}{\pi(\tau_j)}</script>. This formula provides an efficient estimate of the second term of the loss function even though we have only taken a few steps (or perhaps just a <em>single</em> step) in the direction of the “correct” policy. This results in an algorithm which iterative improves estimates of the partition function <script type="math/tex">Z</script>, reward <script type="math/tex">r_\psi</script>, <em>and</em> optimal policy <script type="math/tex">\pi</script> known as  <a href="https://arxiv.org/abs/1603.00448">Guided Cost Learning</a>.</p>

<blockquote>
  <h3 id="algorithm-guided-cost-learning">Algorithm: Guided Cost Learning</h3>
  <ol>
    <li>Generate expert demonstrations, <script type="math/tex">\tau_i</script>.</li>
    <li>Initialize a random policy network <script type="math/tex">\pi_\theta</script> and reward network <script type="math/tex">r_\psi</script>.</li>
    <li>Repeat:
      <ul>
        <li>Run policy <script type="math/tex">\pi_\theta</script> to collect trajectories <script type="math/tex">\tau_j</script>.</li>
        <li>Perform gradient ascent on <script type="math/tex">\psi</script> according to <script type="math/tex">\nabla_\psi \mathcal{L}</script>.</li>
        <li>Update <script type="math/tex">\pi_\theta</script> by running a policy optimization algorithm.</li>
      </ul>
    </li>
  </ol>
</blockquote>

<p>Notice that the algorithm outputs the reward as well as the soft optimal policy associated with that reward. The paper contains a number of details that make things work nicely. For example, the authors use a model-based maximum entropy reinforcement learning algorithm that uses LQR to approximate the dynamics under the optimal policy. That seems important because the algorithm requires us to know the probability of whole trajectories, <script type="math/tex">q(\tau)</script>. We wouldn’t get using model-free methods. They also use some interesting regularization methods to improve their results. Overall, it is a much more complex algorithm than what we might be used to in deep reinforcement learning! Next, we’ll see that guided cost learning is equivalent to a simpler approach that should be more familiar to the general deep learning community.</p>

<p><img src="/assets/img/guided-cost-learning.png" alt="guided-cost-learning" /></p>

<blockquote>
  <p><strong>Comment</strong>: This is similar to an idea from a classical optimal control, namely, value iteration. In value iteration, we alternate between updating an estimate of the value function and an optimal policy with respect to that value function.</p>
</blockquote>

<h2 id="generative-adversarial-imitation-learning">Generative Adversarial Imitation Learning</h2>
<p>Let’s forget inverse reinforcement learning for a minute and return to the idea of imitation learning. We don’t care about the reward function; we just want to do what the expert does. We can think of this as learning to perform series of actions that lead to the same distribution of trajectories, <script type="math/tex">p(\tau)</script>, as the expert. Generative adversarial networks (GANs) <a href="https://arxiv.org/abs/1406.2661">Goodfellow et al., 2014</a> are a now well-know method for generating samples that mimic complicated distribution. For example, you can train a GAN to generate paintings of a particular style, or you can train a (conditional) GAN to plausibly “fill in” missing information in a photo.</p>

<p><img src="/assets/img/gan.png" alt="gan" /></p>

<p>GANs work by playing a game between a “generator” network and a “discriminator” network. The generator tries to trick the discriminator by creating samples that the discriminator can’t tell from the real thing, and the discriminator tries to learn the difference between real samples and fakes. More specifically, let <script type="math/tex">G_{\theta}(z)</script> be the generator network that maps noise <script type="math/tex">z</script> to the output space, and let <script type="math/tex">D_{\psi}(x)</script> be the discriminator that maps the output space to the probability that an example output is real. We train the networks by creating samples that are equal parts real and fake outputs, update <script type="math/tex">D</script> by maximizing the probability of labeling the outputs correctly, and update <script type="math/tex">G</script> by maximizing the probability of labeling the fake outputs <em>incorrectly</em>.</p>

<blockquote>
  <h4 id="algorithm-generative-adversarial-learning">Algorithm: Generative Adversarial Learning</h4>
  <ol>
    <li>Generate samples from the truth distribution, <script type="math/tex">p(x)</script>.</li>
    <li>Randomly initialize networks <script type="math/tex">D_\psi(x)</script> and <script type="math/tex">G_\theta(x)</script>.</li>
    <li>Repeat:
      <ul>
        <li>Generate <script type="math/tex">N</script> samples using <script type="math/tex">G</script> and randomly choose <script type="math/tex">N</script> of the true samples.</li>
        <li>Perform a gradient ascent update on <script type="math/tex">D_\psi</script> according to <script type="math/tex">\nabla_{\psi} \mathcal{L}_D</script>.</li>
        <li>Perform a policy update on <script type="math/tex">G_\theta</script> according to <script type="math/tex">\nabla_{\theta}\mathcal{L}_G</script>.</li>
      </ul>
    </li>
  </ol>
</blockquote>

<p>The loss of the discriminator update is</p>

<script type="math/tex; mode=display">\mathcal{L}_{D} \approx \frac{1}{N} \sum_{i=1}^N \log D_{\psi}(x_i) + \sum_{j=1}^N \log (1 - D_{\psi}(G_{\theta}(z_j))),</script>

<p>the loss of the generator is</p>

<script type="math/tex; mode=display">\mathcal{L}_{G} \approx \frac{1}{N} \sum_{i=1}^N \log D(G_{\theta}(z_i)) - \sum_{i=1}^N \log (1 - D(G_{\theta}(z_i))),</script>

<p>and the updates are:</p>

<script type="math/tex; mode=display">\psi \leftarrow \psi + \eta \nabla_{\psi} \mathcal{L}_D</script>

<script type="math/tex; mode=display">\theta \leftarrow \theta + \eta \nabla_{\theta} \mathcal{L}_G</script>

<p><a href="https://arxiv.org/abs/1606.03476">Ho and Ermon, NIPS 2016</a> showed that GANs provide an effective framework for performing imitation learning. The idea is that the discriminator distinguishes between expert and sample trajectories, while the generator tries to “improve” its policy by making it less likely to be “found out”. Typically the policy is trained using a version of deep policy gradient, but the normal reward is replaced by the probability of generating samples that look like the expert, i.e.</p>

<script type="math/tex; mode=display">\nabla_\theta \mathcal{L}_\pi \approx \frac{1}{M} \nabla_\theta \log \pi_\theta (\tau_j) \log D_\psi (\tau_j)</script>

<blockquote>
  <h4 id="algorithm-generative-adversarial-imitation-learning">Algorithm: Generative Adversarial Imitation Learning</h4>
  <ol>
    <li>Generate samples from an expert policy, <script type="math/tex">\pi^{*}</script>.</li>
    <li>Randomly initialize networks <script type="math/tex">D_{\psi}(\tau)</script> and <script type="math/tex">\pi_{\theta}(a \vert s)</script>.</li>
    <li>Repeat:
      <ul>
        <li>Generate <script type="math/tex">N</script> samples from <script type="math/tex">\pi_{\theta}</script> and choose <script type="math/tex">N</script> of the expert samples.</li>
        <li>Perform a gradient ascent update on <script type="math/tex">D_\psi</script> according to <script type="math/tex">\nabla_{\psi} \mathcal{L}_D</script>.</li>
        <li>Perform a policy update on <script type="math/tex">\pi_\theta</script> according to <script type="math/tex">\nabla_{\theta}\mathcal{L}_\pi</script>.</li>
      </ul>
    </li>
  </ol>
</blockquote>

<p>This is all well and good, but it isn’t inverse reinforcement learning: in the end, we recover a policy that mimics the expert, but we don’t really know how we got there. With a slight modification, however, we can leverage the GAN framework to learn a policy and a reward function. This works as follows.</p>

<p>Suppose that we know the expert distribution <script type="math/tex">l(\tau)</script> and the imitation distribution <script type="math/tex">q(\tau)</script>. What probability should we assign to <script type="math/tex">\tau</script> being generated by the expert? The answer is the conditional probability,</p>

<script type="math/tex; mode=display">D(\tau) = \frac{l(\tau)}{l(\tau) + q(\tau)}</script>

<p>All we need to do is parameterize <script type="math/tex">l(\tau)</script> using the maximum entropy approach:</p>

<script type="math/tex; mode=display">l(\tau) = p(\tau) \frac{1}{Z} \exp \left( r_{\psi}(\tau) \right)</script>

<p>We can also note that the likelihood of the generator is always given by</p>

<script type="math/tex; mode=display">q(\tau) = p(\tau) \prod_t \pi_{\theta}(a_t \vert s_t)</script>

<p>Thus, after canceling out <script type="math/tex">p(\tau)</script>, we find that the optimal discriminator is</p>

<script type="math/tex; mode=display">D(\tau) = \frac{ \frac{1}{Z} \exp \left( r_{\psi}(\tau) \right)}{ \frac{1}{Z} \exp \left( r_{\psi}(\tau) \right) +  \prod_t \pi_{\theta}(a_t \vert s_t)}</script>

<p>The generative adversarial approach to <em>inverse</em> reinforcement learning uses this discriminator and updates the generator policy using <script type="math/tex">r_{\psi}</script> instead of <script type="math/tex">D</script>:</p>

<script type="math/tex; mode=display">\nabla_{\theta} \mathcal{L}_{G} \approx \frac{1}{M} \sum_{j=1}^M \nabla_{\theta} \log \pi_{\theta} (\tau_j) r_{\psi}(\tau_j)</script>

<p>The discriminator’s loss is the same as before:</p>

<script type="math/tex; mode=display">\mathcal{L}_{D} \approx \frac{1}{N} \sum_{i=1}^N \log D(\tau_i) + \frac{1}{N} \sum_{j=1}^N \log (1 - D(\tau_j)),</script>

<p>where <script type="math/tex">\tau_i</script> are trajectories generated by the expert and <script type="math/tex">\tau_j</script> are trajectories generated by the agent.</p>

<blockquote>
  <h4 id="algorithm-generative-adversarial-inverse-reinforcement-learning">Algorithm: Generative Adversarial Inverse Reinforcement Learning</h4>
  <ol>
    <li>Generate samples from an expert policy, <script type="math/tex">\pi^{*}</script>.</li>
    <li>Randomly initialize networks <script type="math/tex">D_{\psi, Z}(\tau)</script> and <script type="math/tex">\pi_{\theta}(a \vert s)</script>.</li>
    <li>Repeat:
      <ul>
        <li>Generate <script type="math/tex">N</script> samples from <script type="math/tex">\pi_{\theta}</script> and choose <script type="math/tex">N</script> of the expert samples.</li>
        <li>Perform a gradient ascent update on <script type="math/tex">D_{\psi, Z}</script> according to <script type="math/tex">\nabla_{\psi, Z} \mathcal{L}_D</script>.</li>
        <li>Perform a policy update on <script type="math/tex">\pi_\theta</script> according to <script type="math/tex">\nabla_{\theta}\mathcal{L}_\pi</script>.</li>
      </ul>
    </li>
  </ol>
</blockquote>

<p>Did you notice any similarity between the figures representing the guided cost learning algorithms and GAN? <a href="https://arxiv.org/abs/1611.03852">Finn et al. 2016</a> shows that this algorithm is infact <em>equivalent</em> to maximum entropy inverse reinforcement learning, and guided cost learning in particular. Notice that the discriminator network has two trainable parameters, <script type="math/tex">\psi</script> and <script type="math/tex">Z</script>. <script type="math/tex">\psi</script> parameterizes the reward function, <script type="math/tex">r_\psi</script>, and <script type="math/tex">Z</script> is a <em>direct</em> estimate of the partition function. In this setting we don’t have to mess with importance sampling because we directly learn <script type="math/tex">Z</script>! Also notice that there are actually two changes required to equate GAN with maximum entropy IRL. First, we assume that we have access to <script type="math/tex">q(\tau)</script> so that we can evaluate <script type="math/tex">D_{\psi, Z}(\tau)</script>. Second, we parameterize <script type="math/tex">l</script> in terms of the reward function, <script type="math/tex">r_\psi(\tau)</script>. These changes allow us to learn the reward function instead of only learning a binary classification probability.</p>

<blockquote>
  <p><strong>Question</strong>: Does it matter whether we use the reward or the discriminator to update <script type="math/tex">\pi_\theta</script>?</p>
</blockquote>

<p>I hope this post has done some justice to this fascinating area of study. The big picture of inverse reinforcement learning is that we want to learn the underlying motivation of agents that we assume behave near optimally. It strikes me that these methods might have applications outside of the robotics domain. For example, researchers could use such methods to develop task-automation tools that mimic user preferences. You can learn more by reading the papers below.</p>

<h2 id="references">References</h2>
<ul>
  <li>Ziebart et al., AAAI 2008. <em>Maximum Entropy Inverse Reinforcement Learning</em>.</li>
  <li>Finn et al., ICML 2016. <em>Guided Cost Learning</em>.</li>
  <li>Goodfellow et al., NIPS 2014. <em>Generative Adversarial Networks</em>.</li>
  <li>Ho and Ermon, NIPS 2016. <em>Generative Adversarial Imitation Learning</em>.</li>
  <li>Finn et al., 2016. <em>A Connection Between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models</em>.</li>
</ul>


        </article>

    </div>

    <div style="position: absolute; width: 200px; top: 0px; left: 860px;">
        <div uk-sticky="offset: 260">
            <ul class="uk-nav uk-nav-default uk-nav-parent-icon tm-nav" uk-scrollspy-nav="closest: li; scroll: true; offset: 100">
                
            </ul>
        </div>
    </div>

    <div id="disqus_thread" class="uk-margin-large-top"></div>
    <script>
        /**
        *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
        *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
        /*
        var disqus_config = function () {
        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };
        */
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = 'https://katabaticwindblog.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

</div>


        </div>

        <div class="uk-section uk-section-xsmall tm-footer">
    <div class="uk-container uk-container-small uk-position-relative">
        <ul class="uk-navbar-nav uk-align-center">
            <li>
                <a href="#">cswaney.github.io</a>
            </li>
            <!-- <li>
                <p class="uk-text-center uk-text-small">Created with <a href="https://getuikit.com" uk-icon="icon: uikit; ratio: 1.0"></a></p>
            </li> -->
        </ul>
    </div>
</div>


        <!-- convert MathJax to KaTex -->
        <script>
          $("script[type='math/tex']").replaceWith(function() {
              var tex = $(this).text();
              return katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: false});
          });

          $("script[type='math/tex; mode=display']").replaceWith(function() {
              var tex = $(this).html();
              return katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: true});
          });
        </script>

    </body>

</html>
