<!DOCTYPE html>
<html>

    <head>
        <title>Deep Inverse Reinforcement Learning</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- theme -->
        <link rel="stylesheet" href="/assets/css/prism.css"/>
        <!-- UIKit -->
        <link rel="stylesheet" href="/assets/css/uikit.min.css"/>
        <script src="/assets/js/uikit.min.js"></script>
        <script src="/assets/js/uikit-icons.min.js"></script>
        <!-- Prism -->
        <link rel="stylesheet" href="/assets/css/theme.css"/>
        <script src="/assets/js/prism.js"></script>
        <!-- GoogleFonts -->
        <link href="https://fonts.googleapis.com/css?family=Lora" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Fjalla+One" rel="stylesheet">
        <!-- KaTeX -->
        <script src="//code.jquery.com/jquery-1.11.1.min.js"></script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css">
        <script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js"></script>
    </head>

    <body class="uk-background uk-height-viewport">

        <div class="tm-navbar-container" uk-sticky="animation: uk-animation-slide-top; sel-target: .uk-navbar-container; cls-active: uk-navbar-sticky; cls-inactive: uk-navbar-transparent; top: 300">

    <nav class="uk-navbar-container" uk-navbar style="position: relative; z-index: 980;">

        <div class="uk-navbar-center">

            <div class="uk-navbar-center-left">
                <div>
                    <ul class="uk-navbar-nav">
                        
                            <li class="uk-active"><a class="tm-navbar-item" href="/index.html">Blog</a></li>
                        

                        
                            <li><a class="tm-navbar-item" href="/info.html">Info</a></li>
                        
                    </ul>
                </div>
            </div>

            <a class="uk-navbar-item uk-logo" href="#">
                <img uk-svg src="/assets/img/penguin.svg" style="width:56px; height: 56px;">
            </a>

            <div class="uk-navbar-center-right">
                <div>
                    <ul class="uk-navbar-nav">
                        <li>
                            <a href="https://github.com/cswaney" uk-icon="icon: github; ratio: 1.5"></a>
                        </li>
                        <li>
                            <a href="https://twitter.com/SwaneyColin?lang=en" uk-icon="icon: twitter; ratio: 1.5"></a>
                        </li>
                    </ul>
                </div>
            </div>

        </div>

    </nav>

</div>


        <div class="uk-section uk-section-default uk-section-small uk-padding-remove-top uk-margin-small-top">

                <div class="uk-container uk-container-small uk-position-relative">

    <div>
        <article class="uk-article">

            <h1 class="uk-article-title"><a class="uk-link-reset" href="">Deep Inverse Reinforcement Learning</a></h1>

            <p class="uk-article-meta tm-article-meta">Written by <a href="#">Colin Swaney</a> on Wednesday, September 4, 2019</a></p>

            <!-- tags... -->

            <p><strong>Note</strong>: This post is based on <a href="http://rail.eecs.berkeley.edu/deeprlcourse-fa18/static/slides/lec-16.pdf">this lecture</a> given by Sergey Levine in his course on deep reinforcement learning at Berkeley in the fall of 2018.</p>

<p>The typical goal of reinforcement learning is to discover optimal—or at least satisfactory—policies to interact with a given environment. A critical part of the environment is the agent’s reward function. It doesn’t really matter whether I know the laws of physics if I don’t know what task I’m meant to achieve. In constrast, the goal of inverse reinforcement learning is to discover the reward function given a policy function.</p>

<p>Imagine that you’ve been shown how to perform a task, for example, how to throw a ball. You have many, many examples of an expert performing this task, but now you need to learn the task. For you or I it might be obvious that the task is to hit a target, but your robot doesn’t have this intuition. One strategy would be to imitate or mimic the expert behavior. That’s a reasonable approach, although there are some <a href="**TODO**">practical difficulties</a> involved. More importantly, it doesn’t actually get to the heart of the matter: we would really like our robot to “understand” her environment so that she can easily learn to perform additional, related tasks.</p>

<p>An alternative approach is for the robot to learn the implicit reward function that the expert policy optimizes. With the its learned reward function, the agent can figure out its own policy and will be able to re-learn optimal policies if its environment is modified in some way. In fact, we can learn the reward function and an associated policy at the same time.</p>

<h2 id="maximum-entropy-inverse-reinforcement-learning">Maximum Entropy Inverse Reinforcement Learning</h2>
<p>Let’s take as a starting point the same setting as the <a href="**TODO**">lecture on optimal policies through probabilistic inference</a>. Namely, image that we observe an expert performing a task: we see what states she visits (<script type="math/tex">s_t</script>), what actions she performs (<script type="math/tex">a_t</script>), as well as the rewards she receives (<script type="math/tex">r_t</script>). We imagine a latent variable <script type="math/tex">\mathcal{O}_t</script> which codes whether each action performed is optimal: <script type="math/tex">\mathcal{O}_t = 1</script> if action <script type="math/tex">a_t</script> is optimal in state <script type="math/tex">s_t</script>, and zero otherwise. We assume that optimal actions are exponentially more likely: <script type="math/tex">p(\mathcal{O}_t \vert s_t, a_t) \propto \exp \left( r(s_t, a_t) \right)</script>.</p>

<p>The first question is, “What is the probability of an optimal  trajectory?” That is, what is <script type="math/tex">p(\tau \vert \mathcal{O}_{1:T})</script>? By simply applying Bayes rule we get that</p>

<script type="math/tex; mode=display">p(\tau \vert \mathcal{O}_{1:T}) \propto p(\tau) p(\mathcal{O}_{1:T} \vert \tau) = p(\tau) \prod_{t=1}^T p(\mathcal{O}_t \vert \tau) = p(\tau) \exp \left( \sum_{t=1}^T r(s_t, a_t) \right)</script>

<p><img src="**TODO**" alt="probabilistic-model" /></p>

<p>Now return to the inverse reinforcement learning setting. We don’t know <script type="math/tex">r(s_t, a_t)</script>; we want to <em>learn</em> <script type="math/tex">r_{\psi}(s_t, a_t)</script> that is consistent with the observed trajectories. The basic way to do that is to maximize the likelihood of the trajectory probability above:</p>

<script type="math/tex; mode=display">\max_{\psi} \frac{1}{N} \sum_{i = 1}^{N} \log p(\tau_i \vert \mathcal{O}_{1:T}, \psi) = \max_{\psi} \frac{1}{N} \sum_{i = 1}^{N}  r_{\psi}(\tau_i) - \log Z</script>

<p>Where did <script type="math/tex">Z</script> come from? Bayes rule tells that <script type="math/tex">p(\tau \vert \mathcal{O}_{1:T})</script> is <em>proportional</em> to <script type="math/tex">p(\tau) \exp \left( \sum_{t=1}^T r(s_t, a_t) \right)</script>. To make it a proper probability, we need to normalize the right-hand side so that it integrates to one. <script type="math/tex">Z</script> is that normalizer (called the <em>partition function</em>), which in this case is given by</p>

<script type="math/tex; mode=display">Z = \int_{\tau} p(\tau) \exp \left( \sum_{t=1}^T r(s_t, a_t) \right) d \tau</script>

<p>This turns out to be the devil in this method. Notice that we are trying to integrate over <em>all</em> the trajectories in the environment. In general, this is impossible. But let’s consider the case in which the number of states and actions are small and countable.</p>

<p>First, let’s take a close look at the gradient of the objective function, <script type="math/tex">\mathcal{L} = \frac{1}{N} \sum_{i = 1}^{N} r_{\psi}(\tau_i) - \log Z</script>:</p>

<script type="math/tex; mode=display">\nabla_{\psi} \mathcal{L} = \frac{1}{N} \sum_{i = 1}^{N} \nabla_{\psi} r_{\psi}(\tau_i) - \frac{1}{Z} \nabla_{\psi} Z = \frac{1}{N} \sum_{i = 1}^{N} \nabla_{\psi} r_{\psi}(\tau_i) - \int_{\tau} \left( \frac{p(\tau) \exp \left( r_{\psi}(\tau) \right)}{Z} \right) \nabla_{\psi} r_{\psi}(\tau) d \tau</script>

<p><strong>Comment</strong>: The loss itself is the expectation of the reward under the expert minus the log of <script type="math/tex">Z</script>. There is actually some intuition here. On the one hand, the expert is assumed to visit state-action pairs that are close to optimal, so we want to encourage our reward function to be high along trajectories the expert visited. On the other hand, we don’t want to go assigning probability all over the place. <script type="math/tex">Z</script> is sort of the “total probability” associated with <script type="math/tex">r_{\psi}</script>—we want to minimize this, so its negative in <script type="math/tex">\mathcal{L}</script>. Also, note that if you know the true reward function, then the gradient of the loss zeros out—the expectations are the same because the policies are the same in that case.</p>

<p>The first term is clearly a sample approximation to the expectation of <script type="math/tex">\nabla_{\psi} r_{\psi}</script> under the expert of “demo” policy (sense each of the <script type="math/tex">\tau_i</script> are generated by the expert). Looking at the second term, we see that it is <em>exactly</em> the expectation of <script type="math/tex">\nabla_{\psi} r_{\psi}</script> under the “optimal” policy <em>according to the reward estimate</em>, <script type="math/tex">r_{\psi}</script>. If we know the dynamics of the system, then we can estimate the latter expectation using a forward-backward inference algorithm. (The main idea is that if the states and actions are countable, then we can infer the conditional probability of each state-action pair, then sum up over all state-action pairs to calculate the integral. For details, see <a href="TOOD">here</a>). Thus, the maximum entropy inverse reinforcement learning algorithms amounts to:</p>

<ol>
  <li>Estimate <script type="math/tex">\int_{\tau} \left( \frac{p(\tau) \exp \left( r_{\psi}(\tau) \right)}{Z} \right) \nabla_{\psi} r_{\psi}(\tau) d \tau</script> using the forwards-backwards algorithm.</li>
  <li>Evaluate <script type="math/tex">\nabla_{\psi} \mathcal{L}</script>.</li>
  <li><script type="math/tex">\psi \leftarrow \psi + \eta \nabla_{\psi} \mathcal{L}</script>.</li>
</ol>

<p><strong>TODO</strong>: Why is this called “maximum entropy IRL”? 1. Linear case. 2. The policy from the PGM is a MaxEnt policy…</p>

<h2 id="guided-cost-learning">Guided Cost Learning</h2>
<p>The maximum entropy approach works well for small problems where state-action pairs can be counted and the system dynamics are known (or can be easily learned). In deep reinforcement learning, we are interested in problems with large, continuous state-actions spaces where the dynamics of the environment are unknown. In these settings, we need a different strategy to estimate the part of the loss function that depends on the partition function, <script type="math/tex">Z</script>. Recall that this term amounted to the expectation of <script type="math/tex">\nabla_{\psi} r_{\psi}(\tau)</script> under the distribution of trajectories generated by the optimal policy corresponding to <script type="math/tex">r_{\psi}</script>. In the probabilistic inference framework, such optimal policies are known as “soft” optimal policies or “maximum entropy policies” because they solve</p>

<script type="math/tex; mode=display">\max_{\theta} \sum_{t=1}^T \mathbb{E}_{\pi(s_t, a_t \vert \theta)} \left[ r(s_t, a_t) \right] + \mathbb{E}_{\pi(s_t, a_t \vert \theta)} \left[ \mathcal{H}(\theta) \right]</script>

<p>This suggests a simple sample estimate: learn an optimal policy under <script type="math/tex">r_{\psi}</script> using any MaxEnt RL algorithm you like (see this <a href="**TODO**">post</a>), sample trajectories <script type="math/tex">\{ \tau_j \}_{j=1}^M</script> from that policy, and plug them into the standard estimate:</p>

<script type="math/tex; mode=display">\mathbb{E}_{} \left[ \nabla_{\psi} r_{\psi}(\tau) \right] \approx \frac{1}{M} \sum_{j=1}^M \nabla_{\psi} r_{\psi}(\tau_j)</script>

<p>This results in a loss function that whose first term is an estimate based on samples from the expert and second term is an estimate based on samples from a soft optimal policy. The problem is that now in order to update <script type="math/tex">\psi</script> we have to run a full MaxEnt RL algorithm! Clearly this is too expensive—imagine solving your favorite Atari environment a thousand times! What if instead of running MaxEnt RL to completion, we simply run a few steps? The resulting policy won’t be the soft optimal one, so we can’t directly plug in trajectories to the estimator above. However, we can use importance samplingt to correct for the bias. In particular, the new estimate will be</p>

<script type="math/tex; mode=display">\frac{1}{\sum_{j} w_j} \sum_{j=1}^M w_j \nabla_{\psi} r_{\psi}(\tau_{j})</script>

<p>where <script type="math/tex">w_j = \frac{\exp \left( r_{\psi}(\tau_j) \right)}{\pi(\tau_j)}</script>. (Here’s a way to understand this estimator. The original thing we wanted to calculate was the expectation of some value assuming a <em>uniform distribution</em> over <script type="math/tex">\tau</script>. Now we weight the trajectories by their probability under <script type="math/tex">\pi</script> and simultaneously divide the value by <script type="math/tex">\pi</script> so that the overall result is unchanged). The resulting algorithm is called <a href="**TODO**">Guided Cost Learning</a> because the updates to <script type="math/tex">\pi</script> gradually lead to the estimates of <script type="math/tex">Z</script> taken from experience that is most relevant.</p>

<p><img src="**TODO**" alt="guided-cost-learning" /></p>

<h3 id="algorithm-guided-cost-learning">Algorithm: Guided Cost Learning</h3>
<p>Given human/expert demos…</p>
<ul>
  <li>Initialize random policy, <script type="math/tex">\pi</script>
Loop…</li>
  <li>Generate samples from <script type="math/tex">\pi</script></li>
  <li>Update rewards <script type="math/tex">r_{\psi}</script> using samples and demos (using <script type="math/tex">\mathcal{L}</script>)</li>
  <li>Update <script type="math/tex">\pi</script> with respect to <script type="math/tex">r_{\psi}</script> using MaxEnt RL algorithm
Output <script type="math/tex">r_{\psi}</script> and <script type="math/tex">\pi</script>.</li>
</ul>

<p>Notice that the algorithm outputs the reward <em>and</em> the (soft) optimal policy associated with that reward. Also, their paper, the authors use a model-based MaxEnt RL algorithm (using LQR) that approximates the dynamics under <script type="math/tex">\pi</script>. That’s important because the algorithm requires us to know the probability of whole trajectories, <script type="math/tex">q(\tau)</script>, which is one of the outputs of such a model-based approach: we wouldn’t get using model-free methods. There are also some regularization tricks involved to make things work nicely… <strong>TODO</strong> In short, this is a highly involved procedure! In the last section we’ll see that it is equivalent to a much simpler approach that is more familiar to the general deep learning community.</p>

<h2 id="generative-adversarial-imitation-learning">Generative Adversarial Imitation Learning</h2>
<p>Let’s forget inverse reinforcement learning for a minute and return to the idea of imitation learning. We don’t care about the reward function; we just want to do what the expert does. We can think of this as learning to perform series of actions that lead to the same distribution of trajectories, <script type="math/tex">p(\tau)</script>, as the expert. Generative adversarial networks (GANs) <a href="**TODO**">Goodfellow et al.</a> are a now well-know method for generating samples that mimic complicated distribution. For example, you can train a GAN to generate paintings of a particular style, or you can train a (conditional) GAN to plausibly “fill in” missing information in a photo.</p>

<p>GANs work by playing a game between a “generator” network and a “discriminator” network. The generator tries to trick the discriminator by creating samples that the discriminator can’t tell from the real thing, and the discriminator tries to learn the difference between real samples and fakes.</p>

<ul>
  <li><a href="**TODO**">Ho and Ermon, NIPS 2016</a></li>
</ul>

<h2 id="outline">Outline</h2>
<ol>
  <li>MaxEnt IRL:</li>
  <li>Guided Cost Learning</li>
  <li>GAN</li>
  <li>Guided Cost Learning = (modified) GAN = MaxEnt Learning</li>
</ol>

<h2 id="references">References</h2>
<ul>
  <li>Ziebart et al., AAAI 2008. <em>Maximum Entropy Inverse Reinforcement Learning</em>.</li>
  <li>Finn et al., ICML 2016. <em>Guided Cost Learning</em>.</li>
  <li>Ho and Ermon, NIPS 2016. * Generative Adversarial Imitation Learning*.</li>
  <li>Finn et al., 2016. <em>A Connection Between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models</em>.</li>
</ul>


        </article>

    </div>

    <div style="position: absolute; width: 200px; top: 0px; left: 860px;">
        <div uk-sticky="offset: 260">
            <ul class="uk-nav uk-nav-default uk-nav-parent-icon tm-nav" uk-scrollspy-nav="closest: li; scroll: true; offset: 100">
                
            </ul>
        </div>
    </div>

    <div id="disqus_thread" class="uk-margin-large-top"></div>
    <script>
        /**
        *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
        *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
        /*
        var disqus_config = function () {
        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };
        */
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = 'https://katabaticwindblog.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

</div>


        </div>

        <div class="uk-section uk-section-xsmall tm-footer">
    <div class="uk-container uk-container-small uk-position-relative">
        <ul class="uk-navbar-nav uk-align-center">
            <li>
                <a href="#">cswaney.github.io</a>
            </li>
            <!-- <li>
                <p class="uk-text-center uk-text-small">Created with <a href="https://getuikit.com" uk-icon="icon: uikit; ratio: 1.0"></a></p>
            </li> -->
        </ul>
    </div>
</div>


        <!-- convert MathJax to KaTex -->
        <script>
          $("script[type='math/tex']").replaceWith(function() {
              var tex = $(this).text();
              return katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: false});
          });

          $("script[type='math/tex; mode=display']").replaceWith(function() {
              var tex = $(this).html();
              return katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: true});
          });
        </script>

    </body>

</html>
