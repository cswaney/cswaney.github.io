<!DOCTYPE html>
<html>

    <head>
        <title>Event-Driven Bayesian Models (Pt. II)</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- theme -->
        <link rel="stylesheet" href="/assets/css/prism.css"/>
        <!-- UIKit -->
        <link rel="stylesheet" href="/assets/css/uikit.min.css"/>
        <script src="/assets/js/uikit.min.js"></script>
        <script src="/assets/js/uikit-icons.min.js"></script>
        <!-- Prism -->
        <link rel="stylesheet" href="/assets/css/theme.css"/>
        <script src="/assets/js/prism.js"></script>
        <!-- GoogleFonts -->
        <link href="https://fonts.googleapis.com/css?family=Lora" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Fjalla+One" rel="stylesheet">
        <!-- KaTeX -->
        <script src="//code.jquery.com/jquery-1.11.1.min.js"></script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css">
        <script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js"></script>
    </head>

    <body class="uk-background uk-height-viewport">

        <div class="tm-navbar-container" uk-sticky="animation: uk-animation-slide-top; sel-target: .uk-navbar-container; cls-active: uk-navbar-sticky; cls-inactive: uk-navbar-transparent; top: 300">

    <nav class="uk-navbar-container" uk-navbar style="position: relative; z-index: 980;">

        <div class="uk-navbar-center">

            <div class="uk-navbar-center-left">
                <div>
                    <ul class="uk-navbar-nav">
                        
                            <li class="uk-active"><a class="tm-navbar-item" href="/index.html">Blog</a></li>
                        

                        
                            <li><a class="tm-navbar-item" href="/info.html">Info</a></li>
                        
                    </ul>
                </div>
            </div>

            <a class="uk-navbar-item uk-logo" href="#">
                <img uk-svg src="/assets/img/penguin.svg" style="width:56px; height: 56px;">
            </a>

            <div class="uk-navbar-center-right">
                <div>
                    <ul class="uk-navbar-nav">
                        <li>
                            <a href="https://github.com/cswaney" uk-icon="icon: github; ratio: 1.5"></a>
                        </li>
                        <li>
                            <a href="https://twitter.com/SwaneyColin?lang=en" uk-icon="icon: twitter; ratio: 1.5"></a>
                        </li>
                    </ul>
                </div>
            </div>

        </div>

    </nav>

</div>


        <div class="uk-section uk-section-default uk-section-small uk-padding-remove-top uk-margin-small-top">

                <div class="uk-container uk-container-small uk-position-relative">

    <div>
        <article class="uk-article">

            <h1 class="uk-article-title"><a class="uk-link-reset" href="">Event-Driven Bayesian Models (Pt. II)</a></h1>

            <p class="uk-article-meta tm-article-meta">Written by <a href="#">Colin Swaney</a> on Friday, May 1, 2020</a></p>

            <!-- tags... -->

            <p>We left off by introducing a model that combines a network model with a temporal model of event arrivals called a Hawkes model. The intensity of the <script type="math/tex">n</script>-th node in this model at time <script type="math/tex">t</script> is given by</p>

<script type="math/tex; mode=display">% <![CDATA[
\lambda_n(t) = \lambda_n^{(0)} + \sum_{s_m < t} A_{c_m, n} \cdot W_{c_m, n} \cdot \theta_{c_m, n} \cdot e^{-\theta_{c_m, n} (t - s_m)}, %]]></script>

<p>where <script type="math/tex">A_{c_m, n} \in \{0, 1\}</script>.</p>

<p>Simulating the model is fairly straight-forward using the Poisson Superposition Principle. First, simulate a network <script type="math/tex">A</script> according to the generative model chosen (see previous lecture for details). Next, simulate independent events on each node according a to Poisson process with intensity given by the background rate, <script type="math/tex">\lambda_{n}^{(0)}</script>. Finally, for each event generated by the background process, recursively generate <em>independent</em> Poisson processes with intensity given by the impulse response functions.</p>

<h2 id="inference">Inference</h2>
<p>Now suppose we observe a sequence of events <script type="math/tex">\{ s_m, c_m \}</script>, where <script type="math/tex">c_m \in \{1, \cdots, N\}</script> represents the node of the event. How can we learn about the model parameters? Let’s take a look at some of the standard approaches.</p>

<h3 id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h3>
<p>The classic approach to is to find the parameters <script type="math/tex">\{ \lambda^{(0)}, A, W, \theta \}</script> that maximize the likelihood of the data. One way to accomplish this is by introducing auxiliary parent variables <script type="math/tex">\omega_m</script> denoting the event that “caused” the <script type="math/tex">m</script>-th event. The processes spawned by the events generated by each parent event are independent, so we can apply the Poisson Superposition Principle to compute the “augmented” likelihood:</p>

<script type="math/tex; mode=display">L( \{s_m, c_m, \omega_m\} \ | \ \lambda^{(0)}, A, W, \theta) = \prod_{n=1}^N \text{Poisson} \left( \lambda_n^{(0)} \right) \prod_{m=1}^M \text{Poisson} \left( h_{c_{\omega_m}, c_m} (s_m - s_{\omega_m}) \right)</script>

<p>To compute the likelihood, we sum over all possible values of each <script type="math/tex">\omega_m</script>:</p>

<script type="math/tex; mode=display">% <![CDATA[
L( \{s_m, c_m, \omega_m\} \ | \ \lambda^{(0)}, A, W, \theta) = \prod_{n=1}^N \text{Poisson} \left( \lambda_n^{(0)} \right) \prod_{m=1}^M \text{Poisson} \left( \sum_{\ \omega_m < m} h_{c_{\omega_m}, c_m} (s_m - s_{\omega_m}) \right) %]]></script>

<p>Unfortunately, this likelihood requires <script type="math/tex">\mathcal{O}(M^2)</script> evaluations of the impulse-response function because <em>every</em> event prior to the <script type="math/tex">m^{th}</script> is a potential parent (despite the fact that the probability decays exponentially). Nonetheless, we can evaluate the likelihood and maximize it numerically using standard methods. (Note as well that all of the model parameters are constrained to be <em>non-negative</em>).</p>

<h2 id="maximum-a-posteriori-estimation-map">Maximum a Posteriori Estimation (MAP)</h2>
<p>An alternative point estimation method is to calculate the parameters that maximize the posterior distribution. Recall that the posterior distribution of a probabilistic model with likelihood <script type="math/tex">p(\mathcal{D} \ | \ \Theta)</script> and prior distribution <script type="math/tex">p(\Theta \ | \ \nu)</script> is given by</p>

<script type="math/tex; mode=display">p(\Theta \ | \ \mathcal{D}, \nu) = \frac{p(\mathcal{D} \ | \ \Theta) p(\Theta \ | \ \nu)}{p(\mathcal{D})},</script>

<p>where <script type="math/tex">\mathcal{D}</script> represents the observed data, and <script type="math/tex">\nu</script> represents the hyperparameters governing the prior distribution. The denominator is a constant with respect to <script type="math/tex">\Theta</script>. Thus, maximizing the posterior distribution is equivalent to maximizing the denominator. We have already seen how to calculate the likelihood, so let’s introduce priors on the model parameters.</p>

<p>When the impulse-response rate is parameterized as an exponential function, it turns out that the conditional marginal distribution of each of model parameters is conjugate with a Gamma prior distribution:</p>

<script type="math/tex; mode=display">\lambda_n^{(0)} \sim \text{Gamma}\left(\alpha_0, \beta_0 \right)</script>

<script type="math/tex; mode=display">W_{n, m} \sim \text{Gamma}\left(\alpha_W, \beta_W \right)</script>

<script type="math/tex; mode=display">\theta_{n, m} \sim \text{Gamma}\left(\alpha_\theta, \beta_\theta \right)</script>

<p>The priors are independent, and the joint prior is therefore the product of the individual priors. With this information, we can again use standard numerical optimization methods to calculate <script type="math/tex">\Theta_{MAP}</script>.</p>

<h2 id="bayesian-inference">Bayesian Inference</h2>
<p>MLE and MAP point estimates provide incomplete pictures of the model parameters. To fully explore the model, we want to sample from the marginal posterior distributions. We can use the Markov Chain Monte Carlo technique known as Gibbs sampling to do so. Gibbs sampling works by sampling from the conditional marginal distributions, which are often tractable, even though the joint distribution is intractable. Using gamma priors allows us to easily sample the conditional marginal distributions: they all follow a Gamma distribution due to conjugacy.</p>

<p>Let’s walk through the priors one variable at a time. Start with the background intensity, <script type="math/tex">\lambda^{(0)}</script>. The marginal prior depends on the length of the observation period <script type="math/tex">T</script>, and the number of events that occur on each channel attributed to the background process,</p>

<script type="math/tex; mode=display">M_{n}^{(0)} = \sum_{m=1}^M \mathbb{I}(c_m = n)\mathbb{I}(\omega_{c_m} = 0)</script>

<p>With these sufficient statistics, the prior is given by</p>

<script type="math/tex; mode=display">\lambda_n^{(0)} \sim \text{Gamma}(\alpha_0 + M_n^{(0)}, \beta_0 + T)</script>

<p>As data accumulates, the mean of this distribution converges to the average of <script type="math/tex">M_n / T</script>, or just the average rate of background events.</p>

<p>Similarly, for the weights we count up the number of events attributed to a given parent channel for each of the <script type="math/tex">N</script> channels</p>

<script type="math/tex; mode=display">M_{n \rightarrow n'} = \sum_{m=1}^M \mathbb{I}(c_{\omega_{m}} = n) \mathbb{I}(c_m  = n'),</script>

<p>and the number of events on each channel,</p>

<script type="math/tex; mode=display">M_{n'} = \sum_{m=1}^M \mathbb{I}(c_m = n')</script>

<p>The posterior is then given by</p>

<script type="math/tex; mode=display">W_{n \rightarrow n'} \sim \text{Gamma}(\alpha_W + M_{n \rightarrow n'}, \beta_0 + M_{n'})</script>

<p>In this case, as data accumulates, the mean of the prior approaches the average fraction of events on channel <script type="math/tex">n'</script> attributed to a parent event on channel <script type="math/tex">n</script>.</p>

<p>Finally, the impulse-response likelihood is also conjugate with a gamma distribution prior. In this case, we need to calculate the average duration between parent and child events:</p>

<script type="math/tex; mode=display">X_{n \rightarrow n'} = \frac{1}{M_{n \rightarrow n'}} \sum_m \mathbb{I}(c_m = n') \mathbb{I}(c_{\omega_m} = n) s_m - s_{\omega_m}</script>

<p>The posterior is then</p>

<script type="math/tex; mode=display">θ_{n \rightarrow n'} \sim \text{Gamma}(\alpha_\theta + M_{n \rightarrow n'}, \beta_\theta + M_{n \rightarrow n'} \cdot X_{n \rightarrow n'})</script>

<p>Note that the denominator of <script type="math/tex">X_{n \rightarrow n'}</script> might be zero (if <script type="math/tex">A_{n \rightarrow n'} = 0</script>, for example). In that case, the we revert to the prior distribution. Intuitively, we haven’t seen any data that can inform us about <script type="math/tex">\theta{n \rightarrow n'}</script>, so our best “information” continues to be our prior beliefs.</p>

<h3 id="sampling-the-network">Sampling the Network</h3>
<p>The Hawkes process and the network model interface through the link matrix <script type="math/tex">A</script>. given <script type="math/tex">A</script>, we can perform Gibbs sampling on the Hawkes process as described above, and we can also perform Gibbs sampling on the network model parameters, as described below. But first, we need to sample <script type="math/tex">A</script> itself.</p>

<p>We know that the posterior of <script type="math/tex">A</script> is Bernoulli, and in fact it has the form</p>

<script type="math/tex; mode=display">p(A_{n \rightarrow n'} \ | \ \{s_m, c_m\}_{m=1}^M, \eta, \nu) = \frac{1}{Z} \times p(\{s_m, c_m\}_{m=1}^M \ | \ A, \eta) \times p(A_{n \rightarrow n'} \ | \ \nu),</script>

<p>where <script type="math/tex">\eta = (\lambda^{0}, W, \theta)</script> and <script type="math/tex">\nu</script> represents the parameters of the network model. This quantity is easy to calculate using the likelihood formula above that integrates over parent variables. The normalization constant, <script type="math/tex">Z</script>, is just the sum of the unnormalized “probabilities”.</p>

<p>Given <script type="math/tex">A</script>, sampling the network model is straight-forward: <script type="math/tex">A</script> is simply the “data” in the network model. (For example, in a Bernoulli network model, <script type="math/tex">\rho</script> has an exact beta posterior distribution, <script type="math/tex">\rho \sim \text{Beta}(\alpha + N_0, \beta + N1)</script>, where <script type="math/tex">N_0</script> is the number of unlinked nodes and <script type="math/tex">N_1</script> is the number of linked nodes).</p>

<h3 id="computational-complexity">Computational Complexity</h3>
<p>Inference via Gibbs sampling doesn’t come cheap. The main bottleneck comes from sampling the auxiliary parent variables, which requires us to compute the intensity at <em>every</em> each previous event. Since the local variables are conditionally independent, we can <em>in principle</em> obtain an <script type="math/tex">\mathcal{O}(M)</script> speed-up by sampling them in parallel. In practice, a more effective</p>

<h2 id="examples">Examples</h2>
<p>Let’s look at some examples of these methods in action. We’ll take a simple model containing just two nodes and a dense (fully-connected) network.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">Revise</span><span class="x">,</span> <span class="n">Distributions</span><span class="x">,</span> <span class="n">Gadfly</span><span class="x">,</span> <span class="n">Distributed</span><span class="x">,</span> <span class="n">PointProcesses</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">λ0</span> <span class="o">=</span> <span class="n">ones</span><span class="x">(</span><span class="n">N</span><span class="x">)</span>
<span class="n">W</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">ones</span><span class="x">(</span><span class="n">N</span><span class="x">,</span> <span class="n">N</span><span class="x">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">ones</span><span class="x">(</span><span class="n">N</span><span class="x">,</span> <span class="n">N</span><span class="x">)</span>
<span class="n">θ</span> <span class="o">=</span> <span class="n">ones</span><span class="x">(</span><span class="n">N</span><span class="x">,</span> <span class="n">N</span><span class="x">)</span>
<span class="n">α0</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">β0</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">κ</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">ν</span> <span class="o">=</span> <span class="n">ones</span><span class="x">(</span><span class="n">N</span><span class="x">,</span> <span class="n">N</span><span class="x">)</span>
<span class="n">αθ</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">βθ</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">DenseNetwork</span><span class="x">(</span><span class="n">N</span><span class="x">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">StandardHawkesProcess</span><span class="x">(</span><span class="n">λ0</span><span class="x">,</span> <span class="n">W</span><span class="x">,</span> <span class="n">A</span><span class="x">,</span> <span class="n">θ</span><span class="x">,</span> <span class="n">N</span><span class="x">,</span> <span class="n">α0</span><span class="x">,</span> <span class="n">β0</span><span class="x">,</span> <span class="n">κ</span><span class="x">,</span> <span class="n">ν</span><span class="x">,</span> <span class="n">αθ</span><span class="x">,</span> <span class="n">βθ</span><span class="x">,</span> <span class="n">net</span><span class="x">)</span>
<span class="n">T</span> <span class="o">=</span> <span class="mf">1000.</span><span class="x">;</span>
<span class="n">events</span><span class="x">,</span> <span class="n">nodes</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">p</span><span class="x">,</span> <span class="n">T</span><span class="x">);</span>
</code></pre></div></div>

<h3 id="mle">MLE</h3>

<h3 id="map">MAP</h3>

<h3 id="mcmc">MCMC</h3>
<p>In order to speed-up the Gibbs sampling algorithm, we take advantage of Julia’s distributed computing capabilities (this still takes quite awhile to run, however).</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">Distributed</span>
<span class="n">addprocs</span><span class="x">(</span><span class="mi">8</span><span class="x">)</span>
<span class="nd">@everywhere</span> <span class="k">using</span> <span class="n">Pkg</span>
<span class="nd">@everywhere</span> <span class="n">Pkg</span><span class="o">.</span><span class="n">activate</span><span class="x">(</span><span class="s">"."</span><span class="x">)</span>
<span class="nd">@everywhere</span> <span class="k">using</span> <span class="n">PointProcesses</span>

<span class="n">M</span> <span class="o">=</span> <span class="mi">2000</span><span class="x">;</span>
<span class="n">λ0</span><span class="x">,</span> <span class="n">W</span><span class="x">,</span> <span class="n">θ</span> <span class="o">=</span> <span class="n">mcmc</span><span class="x">(</span><span class="n">p</span><span class="x">,</span> <span class="x">(</span><span class="n">events</span><span class="x">,</span> <span class="n">nodes</span><span class="x">,</span> <span class="n">T</span><span class="x">),</span> <span class="n">M</span><span class="x">);</span>
</code></pre></div></div>

<p>The resulting samples look alright (see below). It look like we could use a bit more data to tighten things up, but all the posterior distributions place a substantial amount of probability around the true parameter values.</p>

<p><img src="/assets/img/mcmc/background_mcmc.png" alt="Gibbs Sampler: Background Rate" />
<img src="/assets/img/mcmc/weights_mcmc.png" alt="Gibbs Sampler: Weights" />
<img src="/assets/img/mcmc/impulse_response_mcmc.png" alt="Gibbs Sampler: Impulse Response" /></p>


        </article>

    </div>

    <div style="position: absolute; width: 200px; top: 0px; left: 860px;">
        <div uk-sticky="offset: 260">
            <ul class="uk-nav uk-nav-default uk-nav-parent-icon tm-nav" uk-scrollspy-nav="closest: li; scroll: true; offset: 100">
                
            </ul>
        </div>
    </div>

    <div id="disqus_thread" class="uk-margin-large-top"></div>
    <script>
        /**
        *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
        *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
        /*
        var disqus_config = function () {
        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };
        */
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = 'https://katabaticwindblog.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

</div>


        </div>

        <div class="uk-section uk-section-xsmall tm-footer">
    <div class="uk-container uk-container-small uk-position-relative">
        <ul class="uk-navbar-nav uk-align-center">
            <li>
                <a href="#">cswaney.github.io</a>
            </li>
            <!-- <li>
                <p class="uk-text-center uk-text-small">Created with <a href="https://getuikit.com" uk-icon="icon: uikit; ratio: 1.0"></a></p>
            </li> -->
        </ul>
    </div>
</div>


        <!-- convert MathJax to KaTex -->
        <script>
          $("script[type='math/tex']").replaceWith(function() {
              var tex = $(this).text();
              return katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: false});
          });

          $("script[type='math/tex; mode=display']").replaceWith(function() {
              var tex = $(this).html();
              return katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: true});
          });
        </script>

    </body>

</html>
